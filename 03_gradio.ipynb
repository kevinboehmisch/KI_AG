{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde0e325-b72e-4416-9bdb-63d32fdcdd5d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"ressource/ki_ag_main.jpg\" alt=\"Logo\" style=\"width: 500px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4487dd-11bb-4a79-8692-421e990e5c6a",
   "metadata": {},
   "source": [
    "<center><h1>Gradio</h1></center>\n",
    "\n",
    "<center>\n",
    "    Dieses Jupyter Notebook gibt dir eine Einführung, wie man Sprachmodelle mit einem UI Namens \"Gradio\" benutzt. \n",
    "    Falls du nicht so fit in Programmieren bist, kannst du auch gerne ein Sprachmodell deiner Wahl nehmen und dir den gewünschten Code generieren lassen.\n",
    "</center>\n",
    "\n",
    "<!--Dies ist ein Beispieltext, der einige **fett markierte** Wörter enthält. Sie können diesen Text anpassen und weitere **Formatierungen** hinzufügen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3aef09",
   "metadata": {},
   "source": [
    "# Installation\n",
    "Als erstes installiere die folgenden Pakete:\n",
    "- gradio\n",
    "- llama-index\n",
    "- llama-index-llms-ollama\n",
    "- llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec08ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: llama-index in c:\\programdata\\miniconda3\\lib\\site-packages (0.11.23)\n",
      "Requirement already satisfied: llama-index-llms-ollama in c:\\programdata\\miniconda3\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: llama-index-embeddings-ollama in c:\\programdata\\miniconda3\\lib\\site-packages (0.3.1)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (4.6.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.7.2 (from gradio)\n",
      "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (0.27.2)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.15-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (11.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (2.9.2)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.9.9-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\miniconda3\\lib\\site-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.7.2->gradio)\n",
      "  Downloading websockets-15.0-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.3.4)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.23 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.11.23)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.9.48.post4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.10 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.2.16)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.2.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.4.0,>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: ollama>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-llms-ollama) (0.3.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\miniconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.16.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.66.4)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.54.4)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\programdata\\miniconda3\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.4.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.16.0)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.1.4)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (5.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-readers-llama-parse>=0.3.0->llama-index) (0.5.13)\n",
      "Requirement already satisfied: click in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (2.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from click->nltk>3.8.1->llama-index) (0.4.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.23.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.2.0)\n",
      "Downloading gradio-5.20.0-py3-none-any.whl (62.3 MB)\n",
      "   ---------------------------------------- 0.0/62.3 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 22.3/62.3 MB 108.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 46.7/62.3 MB 114.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 62.3/62.3 MB 104.4 MB/s eta 0:00:00\n",
      "Downloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Downloading orjson-3.10.15-cp312-cp312-win_amd64.whl (133 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.9.9-py3-none-win_amd64.whl (11.4 MB)\n",
      "   ---------------------------------------- 0.0/11.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 11.4/11.4 MB 64.5 MB/s eta 0:00:00\n",
      "Downloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n",
      "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading websockets-15.0-cp312-cp312-win_amd64.whl (176 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pydub, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, mdurl, groovy, ffmpy, aiofiles, uvicorn, starlette, markdown-it-py, huggingface-hub, safehttpx, rich, gradio-client, fastapi, typer, gradio\n",
      "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.20.0 gradio-client-1.7.2 groovy-0.1.2 huggingface-hub-0.29.1 markdown-it-py-3.0.0 mdurl-0.1.2 orjson-3.10.15 pydub-0.25.1 python-multipart-0.0.20 rich-13.9.4 ruff-0.9.9 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.46.0 tomlkit-0.13.2 typer-0.15.2 uvicorn-0.34.0 websockets-15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio llama-index llama-index-llms-ollama llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c507bf",
   "metadata": {},
   "source": [
    "# Kapitel 1: Das erste UI\n",
    "\n",
    "In diesem Kapitel werden wir ein kleines Beispiel ausführen und Schritt für Schritt lernen, wie man damit eine grafische Anwendung erstellt. \n",
    "\n",
    "Da wir in einem Jupyter Notebook arbeiten, ist ein zusätzlicher Schritt notwendig, damit Gradio direkt hier verwendet werden kann. Dafür verwenden wir einen sogenannten *Magic-Befehl*, der diese Funktionalität ermöglicht. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "787850f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting notebook (from jupyter)\n",
      "  Downloading notebook-7.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyter-console (from jupyter)\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: nbconvert in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter) (7.16.4)\n",
      "Requirement already satisfied: ipykernel in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: jupyterlab in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter) (4.2.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipywidgets) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipykernel->jupyter) (1.8.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\programdata\\miniconda3\\lib\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\miniconda3\\lib\\site-packages (from ipykernel->jupyter) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\miniconda3\\lib\\site-packages (from ipykernel->jupyter) (6.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipykernel->jupyter) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from ipykernel->jupyter) (6.4.1)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab->jupyter) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab->jupyter) (0.27.2)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab->jupyter) (3.1.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab->jupyter) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab->jupyter) (2.14.2)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab->jupyter) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab->jupyter) (72.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\programdata\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\programdata\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\programdata\\miniconda3\\lib\\site-packages (from nbconvert->jupyter) (1.3.0)\n",
      "Collecting jupyterlab (from jupyter)\n",
      "  Downloading jupyterlab-4.3.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\miniconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.6.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (306)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.21.0)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.13)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.14.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.23.0)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\programdata\\miniconda3\\lib\\site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.20.0)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\miniconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\programdata\\miniconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\programdata\\miniconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.20.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.2.2)\n",
      "Requirement already satisfied: fqdn in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.1)\n",
      "Requirement already satisfied: uri-template in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\miniconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\programdata\\miniconda3\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20240906)\n",
      "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 66.9 MB/s eta 0:00:00\n",
      "Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading notebook-7.3.2-py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 13.2/13.2 MB 103.2 MB/s eta 0:00:00\n",
      "Downloading jupyterlab-4.3.5-py3-none-any.whl (11.7 MB)\n",
      "   ---------------------------------------- 0.0/11.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 11.7/11.7 MB 103.7 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets, jupyter-console, jupyterlab, notebook, jupyter\n",
      "  Attempting uninstall: jupyterlab\n",
      "    Found existing installation: jupyterlab 4.2.5\n",
      "    Uninstalling jupyterlab-4.2.5:\n",
      "      Successfully uninstalled jupyterlab-4.2.5\n",
      "Successfully installed ipywidgets-8.1.5 jupyter-1.1.1 jupyter-console-6.6.3 jupyterlab-4.3.5 jupyterlab-widgets-3.0.13 notebook-7.3.2 widgetsnbextension-4.0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\keboit00\\AppData\\Local\\Temp\\pip-uninstall-p6ntr28w'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "#Magic Befehl für Gradio (wenn man das Noteboook neu startet, immer den Befehl mit ausführen)\n",
    "!pip install --upgrade jupyter ipywidgets # war notwendig damit keine Fehlermeldung mehr angezeigt wurde, jedoch hat es davor die extension trotz Fehlermeldung schon geladen?\n",
    "%load_ext gradio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c0998",
   "metadata": {},
   "source": [
    "Jetzt können wir direkt starten. Zuerst importieren wir Gradio und vergeben einen Alias, indem wir folgenden Code ausführen:\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b014c3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradio extension is already loaded. To reload it, use:\n",
      "  %reload_ext gradio\n"
     ]
    }
   ],
   "source": [
    "#hier Gradio importieren\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a64a0",
   "metadata": {},
   "source": [
    "Das Gute an Gradio ist, dass es viele vorgefertigte Komponenten bereitstellt, die sich relativ einfach in den eigenen Code integrieren lassen. Eine besonders interessante Komponente für uns ist das *ChatInterface*. \n",
    "\n",
    "Dieses Objekt erstellt direkt eine vollständige Chat-UI mit praktischen Funktionen wie dem Absenden von Nachrichten, dem Löschen des Chatverlaufs oder dem Rückgängigmachen der letzten Nachricht.\n",
    "\n",
    "Zusätzlich kann dem Interface eine Funktion übergeben werden, die definiert, wie mit eingehenden und ausgehenden Nachrichten umgegangen wird.\n",
    "\n",
    "### Erstellen einer Dummy-Funktion\n",
    "Zunächst erstellen wir eine Dummy-Funktion mit den folgenden Argumenten:\n",
    "\n",
    "- **message**: Die aktuellste Nachricht aus dem Chat.\n",
    "- **history**: Der gesamte bisherige Chatverlauf.\n",
    "\n",
    "Das Ziel der Dummy-Funktion ist es, die aktuelle Nachricht mit dem Präfix `\"Du hast gesagt: \"` zurückzugeben. Außerdem geben wir den Chatverlauf (als String gecastet) aus, um zu verstehen, wie dieser strukturiert ist.\n",
    "\n",
    "#### Beispiel\n",
    "- **Eingabe**: User: *Hallo Gradio*\n",
    "- **Antwort**: Du hast gesagt: *Hallo Gradio*, Chatverlauf: `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f642be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#erstelle die dummy Funktion\n",
    "def antwort_dummy(message, history): #schreibfehler\n",
    "    return \"Du hast gesagt: \"+ str(message) + str(history)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce624c3",
   "metadata": {},
   "source": [
    "Da die Funktion erstellt wurde, können wir diese jetzt testen. Dafür implementieren wir die Funktion in den nachfolgenden Code.\n",
    "\n",
    "Hier eine kurze Erklärung der Struktur und Komponenten des Gradio-Codes:\n",
    "\n",
    "1. **`gr.Blocks`**:\n",
    "   - **Beschreibung**: Eine flexible Layout-Komponente, mit der man verschiedene Elemente wie Buttons, Eingabefelder, Chatbots usw. in einer strukturierten Benutzeroberfläche anordnen kann.\n",
    "   - **Parameter**:\n",
    "     - `theme=gr.themes.Monochrome()`: Setzt das visuelle Thema auf \"Monochrome\". Gradio bietet verschiedene Themen, die das Erscheinungsbild beeinflussen. Mehr Info: https://www.gradio.app/guides/theming-guide\n",
    "\n",
    "2. **`gr.Chatbot`**:\n",
    "   - **Beschreibung**: Eine vordefinierte Komponente für Chatbots, die Dialoge im Stil einer Chat-Anwendung darstellt.\n",
    "   - **Parameter**:\n",
    "     - `placeholder=\"Simple Bot\"`: Zeigt einen Platzhaltertext im Eingabefeld des Chatbots an, bevor der Benutzer eine Nachricht eingibt.\n",
    "\n",
    "3. **`gr.ChatInterface`**:\n",
    "   - **Beschreibung**: Diese Komponente verbindet eine Funktion (`fn`) mit einem Chatbot und definiert, wie Nachrichten verarbeitet werden.\n",
    "   - **Parameter**:\n",
    "     - `fn=antwort_dummy`: Die Funktion, die auf Nachrichten im Chatbot reagiert. Hier könnte z. B. eine KI oder eine andere Logik implementiert sein.\n",
    "     - `type=\"messages\"`: Gibt an, dass die Nachrichten im Chatbot-Format verarbeitet werden.\n",
    "     - `chatbot=chatbot`: Verknüpft diese Schnittstelle mit der vorher definierten `gr.Chatbot`-Instanz.\n",
    "\n",
    "4. **`demo.launch()`**:\n",
    "   - **Beschreibung**: Startet die Gradio-Anwendung und macht sie lokal oder online verfügbar. Wenn der Code ausgeführt wird, öffnet sich ein Interface, das den definierten Chatbot zeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92c1d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Führe dieses Codebeispiel aus zum testen\n",
    "with gr.Blocks(theme=gr.themes.Monochrome()) as demo:\n",
    "    chatbot = gr.Chatbot(value=[], type=\"messages\") #funktioniert auch ohne type=\"messages\" aber keine Warnmeldung mehr\n",
    "    gr.ChatInterface(fn=antwort_dummy, type=\"messages\", chatbot=chatbot) \n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c48e7",
   "metadata": {},
   "source": [
    "Um die Gradio-Anwendung zu beenden, sollte die Codezeile `demo.close()` ausgeführt werden. \n",
    "\n",
    "Falls dies nicht geschieht, wird bei wiederholter Ausführung von `demo.launch()` eine zweite Gradio-Anwendung auf dem Port `n+1` gestartet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bbb690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7862\n"
     ]
    }
   ],
   "source": [
    "#zum beenden der Anwendung\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5a5b4",
   "metadata": {},
   "source": [
    "# Kapitel 2: Ollama + Gradio\n",
    "\n",
    "Nachdem wir gelernt haben, wie man eine Gradio-Anwendung erstellt und ausführt, verbinden wir diese nun mit *Ollama*. \n",
    "\n",
    "Als ersten Schritt initialisieren wir ein LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35dfbf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "#modell definieren\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=60.0)\n",
    "\n",
    "#testen, ob das Modell funktioniert\n",
    "response = llm.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00a882",
   "metadata": {},
   "source": [
    "Zuerst erstellen wir eine Funktion `antwort(message, history)`, die die Nachricht (`message`) an das Sprachmodell übergibt und die Antwort des Modells als Rückgabewert liefert.\n",
    "\n",
    "### Schritte\n",
    "1. Die Funktion `antwort` nimmt zwei Argumente entgegen:\n",
    "   - **message**: Die aktuelle Nachricht.\n",
    "   - **history**: Der bisherige Chatverlauf.\n",
    "   \n",
    "2. Innerhalb der Funktion wird die Nachricht an das Sprachmodell übergeben.\n",
    "\n",
    "3. Die Antwort des Sprachmodells wird als Rückgabewert geliefert.\n",
    "\n",
    "4. Die Funktion wird anschließend in Gradio integriert, um die Nachrichtenverarbeitung zu ermöglichen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c79302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradio extension is already loaded. To reload it, use:\n",
      "  %reload_ext gradio\n",
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext gradio\n",
    "\n",
    "def antwort_llm(message, history):\n",
    "    #hier die LLM Logik definieren \n",
    "    llm = Ollama(model=\"llama3.2\", request_timeout=60.0)\n",
    "    prompt = str(message)+str(history)\n",
    "    response = llm.complete(prompt)\n",
    "    return str(response) # llamainsdex gibt die Antwort als Objekt aus, deshalb in String umwandeln\n",
    "\n",
    "\n",
    "#Erstelle die Gradio Anwendung \n",
    "with gr.Blocks(theme=gr.themes.Monochrome()) as demo:\n",
    "    chatbot = gr.Chatbot(value=[], type=\"messages\")\n",
    "    gr.ChatInterface(fn=antwort_llm, type=\"messages\", chatbot=chatbot)\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda95782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7863\n"
     ]
    }
   ],
   "source": [
    "#zum beenden der Anwendung\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc45629",
   "metadata": {},
   "source": [
    "Wir haben es geschafft eine Gradio Anwendung mit Ollama zu verknüpfen. Das einzige was uns noch fehlt, ist dafür zu Sorgen, dass das Sprachmodell den gesamten Chatverlauf sehen kann. Versuche nun deinen Code so zu verändern, die History dem Sprachmodell mit übergeben wird. Im folgenden gibt es dafür eine Hilfsfunktion parse_gradio_chat_to_llamaindex.\n",
    "Nutze zudem llm.chat, anstatt llm.complete. \n",
    "\n",
    "Beispiel: https://docs.llamaindex.ai/en/stable/examples/llm/ollama/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08a8a688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradio extension is already loaded. To reload it, use:\n",
      "  %reload_ext gradio\n",
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext gradio\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "\n",
    "def parse_gradio_chat_to_llamaindex(gradio_message, gradio_chat, system_message=None):\n",
    "    \"\"\"\n",
    "    Wandelt einen Gradio-Chatverlauf in das Format für llamaindex um und fügt eine letzte User-Nachricht hinzu.\n",
    "    \n",
    "    :param gradio_chat: Liste von Chat-Einträgen aus Gradio\n",
    "    :param system_message: Optionale Systemnachricht (string), die an den Anfang eingefügt wird\n",
    "    :param gradio_message: Zusätzliche User-Nachricht (string), die als letzte Nachricht hinzugefügt wird\n",
    "    :return: Liste von ChatMessage-Objekten im llamaindex-Format\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # Optionale Systemnachricht hinzufügen\n",
    "    if system_message:\n",
    "        messages.append(ChatMessage(role=\"system\", content=system_message))\n",
    "\n",
    "    # Gradio-Chatverlauf parsen\n",
    "    for chat_entry in gradio_chat:\n",
    "        messages.append(ChatMessage(role=chat_entry['role'], content=chat_entry['content']))\n",
    "\n",
    "    # Zusätzliche User-Nachricht hinzufügen\n",
    "    if gradio_message:\n",
    "        messages.append(ChatMessage(role=\"user\", content=gradio_message))\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def antwort_history_llm(message, history):\n",
    "    #hier die LLM Logik definieren \n",
    "    llm = Ollama(model=\"llama3.2\", request_timeout=60.0)\n",
    "    prompt = parse_gradio_chat_to_llamaindex(message, history, system_message=\"Du bist ein hilfreicher KI-Assistent.\")\n",
    "    response = llm.chat(messages=prompt)\n",
    "    return str(response) # llamainsdex gibt die Antwort als Objekt aus, deshalb in String umwandeln\n",
    "\n",
    "\n",
    "#Erstelle die Gradio Anwendung \n",
    "with gr.Blocks(theme=gr.themes.Monochrome()) as demo:\n",
    "    chatbot = gr.Chatbot(value=[], type=\"messages\")\n",
    "    gr.ChatInterface(fn=antwort_history_llm, type=\"messages\", chatbot=chatbot)\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d970192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7864\n"
     ]
    }
   ],
   "source": [
    "#zum beenden der Anwendung\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92351f00",
   "metadata": {},
   "source": [
    "# Kapitel 3: Multimodal\n",
    "Gradio bietet die Möglichkeit, neben Text auch andere Datenformate als Eingabe zu akzeptieren. In unserem Fall möchten wir, dass der Benutzer im UI eine Textdatei hochladen kann, die eine Frage enthält. Das Sprachmodell soll anschließend die Frage basierend auf dem Inhalt der Datei beantworten.\n",
    "\n",
    "### Szenario\n",
    "- **Textdatei:** `data/secret.txt` enthält einen kurzen Text.\n",
    "- **Frage:** \"Wie lautet das Geheimnis?\"\n",
    "\n",
    "### Herausforderung\n",
    "Das vorhandene Beispiel [hier](https://www.gradio.app/guides/chatinterface-examples#llama-index) verwendet die OpenAI-API, während wir Ollama nutzen möchten. Finde nun eine Möglichkeit Ollama zu integrieren.\n",
    "\n",
    "### Tipp\n",
    "Schau dir das Notebook `02_llama_index.ipynb` an, um mehr über die Integration von Ollama zu lernen. Achte darauf, Ollama anstelle von OpenAI in die bestehende Struktur zu integrieren.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1096a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hier ein Multimpodales Gradio UI erstellen mir Ollama Integration\n",
    "\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "\n",
    "\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large\")\n",
    "Settings.llm = Ollama(model=\"llama3.2\") # hier das Ollama LLM LLM als Standart definieren\n",
    "\n",
    "\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def parse_gradio_chat_to_llamaindex(gradio_message, gradio_chat, system_message=None):\n",
    "    \"\"\"\n",
    "    Wandelt einen Gradio-Chatverlauf in das Format für llamaindex um und fügt eine letzte User-Nachricht hinzu.\n",
    "    \n",
    "    :param gradio_chat: Liste von Chat-Einträgen aus Gradio\n",
    "    :param system_message: Optionale Systemnachricht (string), die an den Anfang eingefügt wird\n",
    "    :param gradio_message: Zusätzliche User-Nachricht (string), die als letzte Nachricht hinzugefügt wird\n",
    "    :return: Liste von ChatMessage-Objekten im llamaindex-Format\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # Optionale Systemnachricht hinzufügen\n",
    "    if system_message:\n",
    "        messages.append(ChatMessage(role=\"system\", content=system_message))\n",
    "\n",
    "    # Gradio-Chatverlauf parsen\n",
    "    for chat_entry in gradio_chat:\n",
    "        messages.append(ChatMessage(role=chat_entry['role'], content=chat_entry['content']))\n",
    "\n",
    "    # Zusätzliche User-Nachricht hinzufügen\n",
    "    if gradio_message:\n",
    "        messages.append(ChatMessage(role=\"user\", content=gradio_message))\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def antwort_history_llm(message, history):\n",
    "    #hier die LLM Logik definieren \n",
    "    llm = Ollama(model=\"llama3.2\", request_timeout=60.0)\n",
    "    prompt = parse_gradio_chat_to_llamaindex(message, history, system_message=\"Du bist ein hilfreicher KI-Assistent.\")\n",
    "    response = llm.chat(messages=prompt)\n",
    "    return str(response) # llamainsdex gibt die Antwort als Objekt aus, deshalb in String umwandeln\n",
    "\n",
    "def answer(message, history):\n",
    "    files = []\n",
    "    for msg in history:\n",
    "        if msg['role'] == \"user\" and isinstance(msg['content'], tuple):\n",
    "            files.append(msg['content'][0])\n",
    "    for file in message[\"files\"]:\n",
    "        files.append(file)\n",
    "\n",
    "    documents = SimpleDirectoryReader(input_files=files).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    query_engine = index.as_query_engine()\n",
    "    return str(query_engine.query(message[\"text\"]))\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Monochrome()) as demo:\n",
    "    chatbot = gr.Chatbot(value=[], type=\"messages\")\n",
    "    gr.ChatInterface(\n",
    "        fn=answer, \n",
    "        type=\"messages\",          \n",
    "        chatbot=chatbot,          \n",
    "        description=\"Upload any text or pdf files and ask questions about them!\",             \n",
    "        textbox=gr.MultimodalTextbox(file_types=[\".pdf\", \".txt\"]),\n",
    "        multimodal=True\n",
    "    )\n",
    "    \n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577a436-e129-4e3e-ae09-4a46b3372c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825778d-c829-4a50-8def-ed1d0f92d1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
