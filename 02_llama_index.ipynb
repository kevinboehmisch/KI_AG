{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde0e325-b72e-4416-9bdb-63d32fdcdd5d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"ressource/ki_ag_main.jpg\" alt=\"Logo\" style=\"width: 500px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4487dd-11bb-4a79-8692-421e990e5c6a",
   "metadata": {},
   "source": [
    "<center><h1>Llamaindex</h1></center>\n",
    "\n",
    "<center>\n",
    "    Dieses Jupyter Notebook gibt dir eine Einführung in das Framework \"LLamaindex\". \n",
    "    Falls du nicht so fit in Programmieren bist, kannst du auch gerne ein Sprachmodell deiner Wahl nehmen und dir den gewünschten Code generieren lassen.\n",
    "    Im Vergelich zur ersten Veranstaltung ist diese wesentlich offener und es wird lediglich ein grober Rahmen festgelegt und die Grundsätzlichen Komponenten erklärt.\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3aef09",
   "metadata": {},
   "source": [
    "# Installation\n",
    "Als erstes installiere die folgenden Pakete:\n",
    "- llama-index\n",
    "- llama-index-llms-ollama\n",
    "- llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec08ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\programdata\\miniconda3\\lib\\site-packages (0.11.23)\n",
      "Requirement already satisfied: llama-index-llms-ollama in c:\\programdata\\miniconda3\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: llama-index-embeddings-ollama in c:\\programdata\\miniconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.4.0,>=0.3.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.3.4)\n",
      "Requirement already satisfied: llama-index-cli<0.4.0,>=0.3.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.23 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.11.23)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.3.0,>=0.2.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.9.48.post4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.10 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.2.16)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.2.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.3.0,>=0.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.3.0,>=0.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.2.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.4.0,>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: ollama>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-llms-ollama) (0.3.3)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.54.4)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\programdata\\miniconda3\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2024.10.0)\n",
      "Requirement already satisfied: httpx in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (3.4.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (11.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.16.0)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index) (0.1.4)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (5.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from llama-index-readers-llama-parse>=0.3.0->llama-index) (0.5.13)\n",
      "Requirement already satisfied: click in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.4.0,>=0.3.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (4.6.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from click->nltk>3.8.1->llama-index) (0.4.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (0.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\programdata\\miniconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.23->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index) (3.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.23->llama-index) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.23->llama-index) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index llama-index-llms-ollama llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c507bf",
   "metadata": {},
   "source": [
    "# Kapitel 1: Daten auslesen\n",
    "\n",
    "LlamaIndex bietet zahlreiche Möglichkeiten, Daten auszulesen und diese für das Sprachmodell nutzbar zu machen. Eine verbreitete Methode, um Text aus Dateien auszulesen, ist der **SimpleDirectoryReader**.\n",
    "\n",
    "Im Verzeichnis, in dem sich dieses Jupyter-Notebook befindet, gibt es den Ordner `data/`. In diesem Ordner liegt die PDF-Datei *Datenfusion.pdf*. Informiere dich in der Dokumentation darüber, wie du diesen Ordner und die darin enthaltene PDF-Datei mit dem SimpleDirectoryReader auslesen kannst:  \n",
    "[Dokumentation zum SimpleDirectoryReader](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "776fe17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lese den Ordner \"data/\" hier ein\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"data/\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07ecb6",
   "metadata": {},
   "source": [
    "Nun gib als Output die ersten 5 Elemente aus. Öffne dann die PDF Datei *Datenfusion.pdf* und vergleiche den Output mit den ersten Seiten der PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d445cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='d9d0a0ae-0bb3-4a90-8e98-0e97157c033e', embedding=None, metadata={'page_label': '1', 'file_name': 'Datenfusion.pdf', 'file_path': 'Z:\\\\KI AG\\\\data\\\\Datenfusion.pdf', 'file_type': 'application/pdf', 'file_size': 2810997, 'creation_date': '2025-03-03', 'last_modified_date': '2025-03-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Datenfusion\\nDruckfassung der Vorlesung Datenfusion\\nHochschule Esslingen – Fakultät Informatik und Informationstechnik\\nProf. Dr.-Ing. R. Marchthaler\\nSS 2024\\nVersion: 22. März 2024\\nInhaltsverzeichnis\\nI Einleitung 4\\n1 Informationen zum Kurs 4\\n1.1 Kursaufbau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 Skript/Literatur . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.3 Notationen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2 Überblick 5\\n2.1 Einführendes Beispiel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Beispiel Abhängigkeiten . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.3 Beispiel Schätzung unbekannter Größen . . . . . . . . . . . . . . . . . . . . . . 7\\n2.4 Zusammenfassung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\nII Grundlagen 9\\n3 Wahrscheinlichkeitstheorie 9\\n3.1 Definitionen und Begriffe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.2 Dichtefunktion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.3 Momente und zentrale Momente . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.4 Wichtige empirische Kenngrößen . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.5 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4 Signaltheorie 20\\n4.1 Stochastischer Prozess . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4.2 Autokorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n4.3 Kreuzkorrelation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n4.4 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0081fcc1-b9c2-4f5d-9d4b-bf6b0440fdb3', embedding=None, metadata={'page_label': '2', 'file_name': 'Datenfusion.pdf', 'file_path': 'Z:\\\\KI AG\\\\data\\\\Datenfusion.pdf', 'file_type': 'application/pdf', 'file_size': 2810997, 'creation_date': '2025-03-03', 'last_modified_date': '2025-03-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Vorlesung Datenfusion Inhaltsverzeichnis © 2024 Prof. Dr. R. Marchthaler\\nIII Datenfusion zeitinvarianter Größen 29\\n5 Datenfusion zeitinvariante Größen 29\\n5.1 Schätzung Skalar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n5.2 Schätzung Vektor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5.3 Gewichtete Schätzungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n5.4 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\nIV Kalman-Filter 40\\n6 Zeitvariante Größen 40\\n6.1 Zustandsraum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n6.2 Systemeigenschaften . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n6.3 Zeitdiskrete Systeme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n6.4 System- und Messrauschen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n6.5 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n7 Klassisches Kalman-Filter 54\\n7.1 Herleitung Filtergleichungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n7.2 Beispiel 1: Beta-Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n7.3 Kochrezept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n7.4 Beispiel 2: Alpha-Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n7.5 Beispiel 3: Gamma-Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\n7.6 Modellierung Systemrauschens . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\\n7.7 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\\nV ROSE-Filter 83\\n8 ROSE-Filter 83\\n8.1 Kalman-Verstärkung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\\n8.2 Aufbau ROSE-Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n8.3 Beispiel ROSE-Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\\nVI Nichtlineare-Filter 89\\n9 Spezielle Gauß-Filter 89\\n9.1 EKF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\\n9.2 UKF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\\n9.3 IF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\\n9.4 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\\n10 Partikelfilter 92\\n10.1 Algorithmus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n10.2 Bsp: nichtlin. Funktion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\\n10.3 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\\n-2-\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5f6479a9-ef1c-4b7e-9fc6-87476decdc78', embedding=None, metadata={'page_label': '3', 'file_name': 'Datenfusion.pdf', 'file_path': 'Z:\\\\KI AG\\\\data\\\\Datenfusion.pdf', 'file_type': 'application/pdf', 'file_size': 2810997, 'creation_date': '2025-03-03', 'last_modified_date': '2025-03-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Vorlesung Datenfusion Inhaltsverzeichnis © 2024 Prof. Dr. R. Marchthaler\\nVII Markov-Ketten 100\\n11 Grundlagen 100\\n11.1 Deterministischer Automat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\\n11.2 Bedingte Wahrscheinlichkeit . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\\n12 Markov-Kette (Nichtdeterministischer Automat) 103\\n12.1 Zeitdiskrete Markov-Kette . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n12.2 Zeitkontinuierliche Markov-Kette . . . . . . . . . . . . . . . . . . . . . . . . . . 105\\n12.3 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\\n13 Hidden-Markov-Kette (Hidden-Markov-Modell) 108\\n13.1 Allgemeines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\\n13.2 Schätzung der unbekannten Zustände . . . . . . . . . . . . . . . . . . . . . . . . 109\\n13.3 Ausblick . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\n13.4 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\\nVIII Anhang 112\\nVektor- und Matrizenrechnung 112\\nLiteratur 118 Folie 0.2\\n-3-\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='304801d5-52ed-465e-9e43-3c7cb631e147', embedding=None, metadata={'page_label': '4', 'file_name': 'Datenfusion.pdf', 'file_path': 'Z:\\\\KI AG\\\\data\\\\Datenfusion.pdf', 'file_type': 'application/pdf', 'file_size': 2810997, 'creation_date': '2025-03-03', 'last_modified_date': '2025-03-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Vorlesung Datenfusion Kap. 1 © 2024 Prof. Dr. R. Marchthaler\\nTeil I\\nEinleitung\\n1 Informationen zum Kurs\\n1.1 Aufbau des Kurses\\nBausteine des Kurses\\n• Klassische „Frontalvorlesung“ im Seminarstil\\nI. Einleitung\\nII. Grundlagen\\nIII. Datenfusion zeitinvarianter Größen\\nIV. Kalman-Filter\\nV. ROSE-Filter\\nVI. Nichtlineare-Filter\\nVII. Markov-Ketten\\n• Übungen/Labor\\nFolie 1.4\\n1.2 Skript/Literatur\\nUnterrichtmaterial, Literaturempfehlung und weiterführende Literatur\\nUnterrichtmaterial\\n• Skript in moodle Kurs „Datenfusion“.\\n• Selbsteinschreibung, PW: „Dtnfsn“\\nLiteraturempfehlung und weiterführende Literatur\\n1. Richard E. Neapolitan:„Learning Bayesian Networks“. Prentice Hall, 2003\\n2. Olle Häggström: „Finite Markov Chains and Algorithmic Applications“. Cambridge\\nUniversity Press, 2002\\n3. Brown, R. G.; Hwang, P. Y. C.:„Introduction to Random Signals and Applied Kalman\\nFiltering“. Third Edition. John Wiley & Sons, Inc., 2012\\n4. Simon, Dan:„Optimal State Estimation“, John Wiley & Sons, Inc., 2006\\n5. Thrun, S.; u.a.:„Probabilistic Robotics“, MIT Press, 2005\\n6. Marchthaler, R.; Dingler S.:„Kalman-Filter - Einführung in die Zustandsschätzung und\\nihre Anwendung für eingebettete Systeme“, Springer-Vieweg Verlag, Wiesbaden, 2017\\nhttps://link.springer.com/content/pdf/10.1007%2F978-3-658-16728-8.pdf\\nFolie 1.6\\n-4-\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8ed9860e-5363-476d-af42-9cb403b16d44', embedding=None, metadata={'page_label': '5', 'file_name': 'Datenfusion.pdf', 'file_path': 'Z:\\\\KI AG\\\\data\\\\Datenfusion.pdf', 'file_type': 'application/pdf', 'file_size': 2810997, 'creation_date': '2025-03-03', 'last_modified_date': '2025-03-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Vorlesung Datenfusion Kap. 2 © 2024 Prof. Dr. R. Marchthaler\\n1.3 Notationen\\nNotationen / Definitionen\\nBezeichnung Bedeutung\\nˆx geschätzte bzw. prädizierte Größe\\n˜x korrigierte Größe\\nA,B,... Vektoren, Matrizen\\nAT,BT,... transponierte Vektoren, Matrizen\\nAd,B d,... zeitdiskrete Vektoren, Matrizen\\n0 Nullmatrix\\nI Einheitsmatrix\\nDet(A) Determinante einer Matrix\\nAdj(A) Adjunkte einer Matrix\\nTr(A) Spur einer Matrix\\nE(X) Erwartungswert der Zufallsvariable X\\nVar(X) Varianz der Zufallsvariable X\\nCov(X,Y ) Kovarianz der Zufallsvariable X und Y\\nRang(S) Rang einer Matrix\\nFolie 1.8\\n2 Überblick\\n2.1 Einführendes Beispiel\\nEinwohner in Esslingen\\nWie viele Einwohner hat Esslingen?\\n• Schreiben Sie Ihren Tipp auf einen Zettel\\n• Es liegen nun mehrere Schätzungen über die Anzahl der Einwohner vor.\\nWie lassen sich diese Daten zu einer besseren Schätzungfusionieren?\\n• Lösungsidee:\\n• Zusatzfrage: Um wie viel wird die fusionierte Schätzung mit jeder weiteren Schätzung\\nbesser?\\n-5-\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "#Hier die ersten 5 Elemente ausgeben\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e73bf",
   "metadata": {},
   "source": [
    "Jetzt kann man nicht nur Dateien auslesen, sondern es gibt auch die Möglichkeit, Webseiten auszulesen.\n",
    "\n",
    "Versuche dabei, die folgende Wikipedia-Seite auszulesen: [Effektiver Akzelerationismus](https://de.wikipedia.org/wiki/Effektiver_Akzelerationismus) mit dem `SimpleWebPageReader`.\n",
    "\n",
    "Dokumentation: [https://docs.llamaindex.ai/en/stable/examples/data_connectors/WebPageDemo/](https://docs.llamaindex.ai/en/stable/examples/data_connectors/WebPageDemo/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2dc6e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[Document(id_='https://de.wikipedia.org/wiki/Effektiver_Akzelerationismus', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='# Effektiver Akzelerationismus\\n\\naus Wikipedia, der freien Enzyklopädie\\n\\nZur Navigation springen Zur Suche springen\\n\\nDer **Effektive Akzelerationismus** ([englisch](/wiki/Englische_Sprache\\n\"Englische Sprache\"): _Effective accelerationism_), abgekürzt **e/acc** , ist\\neine in den 2020er Jahren im [Internet](/wiki/Internet \"Internet\") entstandene\\n[philosophische](/wiki/Philosophie \"Philosophie\") Bewegung. Anhänger der\\nPhilosophie möchten eine möglichst schnelle und unbegrenzte Entfaltung von\\ntechnologischem Fortschritt, besonders von [künstlicher\\nIntelligenz](/wiki/K%C3%BCnstliche_Intelligenz \"Künstliche Intelligenz\") (KI).\\nDie Bewegung hat [utopische](/wiki/Utopie \"Utopie\") Untertöne und\\nargumentiert, dass die Menschheit ihre technologische Entwicklung\\nbeschleunigen sollte, um [Bewusstsein](/wiki/Bewusstsein \"Bewusstsein\") im\\ngesamten Kosmos zu verbreiten und damit den „[thermodynamischen\\nWillen](/wiki/Thermodynamisch \"Thermodynamisch\")“ des Universums zu\\nverwirklichen. Anfangs auf das Internet beschränkt, wurde später auch in\\n[Leitmedien](/wiki/Leitmedium \"Leitmedium\") über die Bewegung und ihre Ideen\\nberichtet.[1]\\n\\n## Inhaltsverzeichnis\\n\\n  * 1 Grundüberzeugungen\\n  * 2 Ursprung und Geschichte\\n  * 3 Rezeption\\n  * 4 Siehe auch\\n  * 5 Weblinks\\n  * 6 Einzelnachweise\\n\\n## Grundüberzeugungen\\n\\n[[Bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&veaction=edit&section=1 \"Abschnitt bearbeiten: Grundüberzeugungen\") | [Quelltext bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&action=edit&section=1 \"Quellcode des Abschnitts bearbeiten: Grundüberzeugungen\")]\\n\\ne/acc ist dem Technologieoptimismus zuzuordnen, welcher technologischen\\nFortschritt als nahezu uneingeschränkt positiv ansieht. Die Bewegung grenzt\\nsich damit vom [Luddismus](/wiki/Luddismus \"Luddismus\"), bzw. neueren\\nAusprägungen davon ab, welcher versucht, technologische Neuerungen aufzuhalten\\noder zu beschränken. Anhänger von [Degrowth](/wiki/Degrowth \"Degrowth\") oder\\nähnlichen Bewegungen werden häufig von ihr als „decels“ (von _Deceleration_)\\nverspottet. Auch von dem [Effektiven Altruismus](/wiki/Effektiver_Altruismus\\n\"Effektiver Altruismus\") bzw. [Longtermism](/wiki/Longtermism \"Longtermism\")\\ngrenzt sich e/acc ab, auch wenn einige philosophische Grundpositionen und\\nZiele ähnlich sind.[2] Da zahlreiche Anhänger des Effektiven Altruismus sich\\nfür stärkere staatliche Regulierung von KI einsetzten, werden sie von\\nAnhängern von e/acc als „Doomer“ (von _Doom_) bezeichnet. Der effektive\\nAkzelerationismus enthält Elemente älterer [Silicon-\\nValley](/wiki/Silicon_Valley \"Silicon Valley\")-Subkulturen wie dem\\n[Transhumanismus](/wiki/Transhumanismus \"Transhumanismus\") und den Ideen von\\n[Ray Kurzweil](/wiki/Ray_Kurzweil \"Ray Kurzweil\"), die in ähnlicher Weise den\\nWert des Fortschritts betonten, sowie der Arbeit der [Cybernetic Culture\\nResearch\\nUnit](/w/index.php?title=Cybernetic_Culture_Research_Unit&action=edit&redlink=1\\n\"Cybernetic Culture Research Unit \\\\(Seite nicht vorhanden\\\\)\") von [Nick\\nLand](/wiki/Nick_Land \"Nick Land\").[3][1][4]\\n\\nLaut Guillaume Verdon, einem der Gründer der Bewegung, ist es das Ziel der\\nmenschlichen Zivilisation, „den Kardashev-Gradienten zu erklimmen“, was\\nbedeutet, dass die menschliche Zivilisation auf der [Kardashev-\\nSkala](/wiki/Kardaschow-Skala \"Kardaschow-Skala\") auf die nächste Stufe\\naufsteigt, indem sie die Energienutzung maximiert. Ein Schlüssel zur\\nErreichung dieses Ziels ist die Entwicklung von [Artificial General\\nIntelligence](/wiki/Artificial_General_Intelligence \"Artificial General\\nIntelligence\") (AGI) und [Superintelligenz](/wiki/Superintelligenz\\n\"Superintelligenz\"). Die Verlangsamung und die Verhinderung von solcher\\nTechnologie sehen Anhänger der Bewegung als Katastrophe an. Sie glauben häufig\\nnicht an [existenzielle KI-\\nRisiken](/wiki/Existenzielles_Risiko_durch_k%C3%BCnstliche_Intelligenz\\n\"Existenzielles Risiko durch künstliche Intelligenz\") oder sehen diese als\\nvernachlässigbar gegenüber den Chancen der Technologie. Auch eine starke\\nRegulierung der Technologie durch Regierungen und eine Beschränkung des freien\\nMarktes wird weitgehend abgelehnt. KI sollte stattdessen möglichst dezentral\\nentwickelt werden und das [AI-Alignment](/wiki/AI-Alignment \"AI-Alignment\")\\ndem Markt überlassen werden.[4] Das [kapitalistische](/wiki/Kapitalismus\\n\"Kapitalismus\") Marktsystem wird dabei selbst als eine Form von Intelligenz\\ngesehen, welche dem Wachstum der Zivilisation dient. Deshalb ist die\\nBeschleunigung des „Technokapitals“ das Ziel der Bewegung. Bürokratische und\\ntechnokratische Kontrolle und zentrale Steuerung von komplexen Systemen wird\\nabgelehnt. Stattdessen würde ein selbststeuerndes, dynamisches System mit\\nhoher Varianz auch zum „besten“ Ergebnis für das Bewusstsein im Universum\\nführen.[5]\\n\\nDie Begründer der Bewegung sehen ihre Wurzeln in [Jeremy\\nEnglands](/w/index.php?title=Jeremy_Englands&action=edit&redlink=1 \"Jeremy\\nEnglands \\\\(Seite nicht vorhanden\\\\)\") Theorie über den [Ursprung des\\nLebens](/wiki/Ursprung_des_Lebens \"Ursprung des Lebens\"), die auf\\n[Entropie](/wiki/Entropie \"Entropie\") und [Thermodynamik](/wiki/Thermodynamik\\n\"Thermodynamik\") Bezug nimmt.[4] Effektiver Akzelerationismus zielt darauf ab,\\ndem \"Willen des Universums\" zu folgen. Das bedeutet \"sich an die\\nthermodynamische Tendenz zu einer Zukunft mit größeren und intelligenteren\\nZivilisationen anzulehnen, welche effektiver darin sind, freie Energie aus dem\\nUniversum zu finden/zu extrahieren und sie in immer größerem Maßstab in Nutzen\\numzuwandeln\".[5] Indem sich das Leben im gesamten Universum ausbreitet und das\\nLeben immer mehr Energie verbraucht, würde der Zweck des Universums\\nerfüllt.[4] Verdon sieht in technologischem Fortschritt und der kosmischen\\nExpansion die Möglichkeit, \"die nächste Evolution des Bewusstseins einzuleiten\\nund unvorstellbare Lebensformen der nächsten Generation zu schaffen.[4] Laut\\nVerdon empfindet e/acc dabei \"keine besondere Loyalität gegenüber biologischen\\nSubstraten für Intelligenz und Leben\". Hauptsache sei, dass sich Bewusstsein\\nund Intelligenz im Universum ausbreite.[6]\\n\\n## Ursprung und Geschichte\\n\\n[[Bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&veaction=edit&section=2 \"Abschnitt bearbeiten: Ursprung und Geschichte\") | [Quelltext bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&action=edit&section=2 \"Quellcode des Abschnitts bearbeiten: Ursprung und Geschichte\")]\\n\\nWährend Nick Land als der intellektuelle Urheber des zeitgenössischen\\n[Akzelerationismus](/wiki/Akzelerationismus \"Akzelerationismus\") im\\nAllgemeinen gilt, bleiben die genauen Ursprünge des effektiven\\nAkzelerationismus unklar. Als Inspiration dienten Technologieunternehmer wie\\n[Elon Musk](/wiki/Elon_Musk \"Elon Musk\") oder [Jeff Bezos](/wiki/Jeff_Bezos\\n\"Jeff Bezos\"), welche beide eigene Raumfahrtunternehmen gegründet haben. Bezos\\nverkündete, er wünsche „sich eine Billion Menschen in unserer Galaxie, die auf\\nriesigen Raumstationen leben“ und dass es „tausend [Mozarts](/wiki/Mozart\\n\"Mozart\") und tausend [Einsteins](/wiki/Albert_Einstein \"Albert Einstein\")“\\ngäbe.[7]\\n\\nIdeen der Bewegung wurden auf der Plattform [Twitter](/wiki/Twitter \"Twitter\")\\nvon anonymen Accounts verbreitet. Der früheste bekannte Hinweis auf die\\nBewegung geht auf einen Newsletter zurück, der im Mai 2022 von vier\\npseudonymen Autoren auf der Bloggingplattform [Substack](/wiki/Substack\\n\"Substack\") veröffentlicht wurde, die unter ihren X (früher\\nTwitter)-Benutzernamen @BasedBeffJezos, @bayeslord, @zestular und\\n@creatine_cycle bekannt sind.[8][2] Im Juli 2022 veröffentlichten „Beff Jezos“\\nund „Bayeslord“ die „Prinzipien und Grundsätze“ von e/acc.[5]\\n_[Forbes](/wiki/Forbes \"Forbes\")_ enthüllte im Dezember 2023, dass die Persona\\n@BasedBeffJezos von Guillaume Verdon, einem kanadischen ehemaligen\\n[Google](/wiki/Google \"Google\")-Ingenieur für\\n[Quantencomputer](/wiki/Quantencomputer \"Quantencomputer\") und theoretischen\\nPhysiker, unterhalten wird. Die Zeitschrift begründete ihre Entscheidung,\\nVerdons Identität preiszugeben, damit, dass dies „im öffentlichen Interesse“\\nliege.[9]\\n\\nDas _Techno-Optimist Manifesto_ („Das Techno-Optimistische Manifest“) des\\nSilicon-Valley-Investors [Marc Andreessen](/wiki/Marc_Andreessen \"Marc\\nAndreessen\") im Oktober 2023 wurde mit e/acc in Verbindung gebracht.[10][11]\\nEs spricht sich für die Beschleunigung von „Technokapital“ aus und bezeichnet\\nTechnologie als befreiend für „das menschliche Potenzial, die menschliche\\nSeele und den menschlichen Geist“. Als Feinde des Fortschritts identifiziert\\ner dabei [Nietzsches](/wiki/Friedrich_Nietzsche \"Friedrich Nietzsche\")\\n„[Letzten Menschen](/wiki/Letzter_Mensch_\\\\(Nietzsche\\\\) \"Letzter Mensch\\n\\\\(Nietzsche\\\\)\")“ und schlechte Ideen, darunter fallen für ihn u. a. die\\n[Sustainable Development Goals](/wiki/Ziele_f%C3%BCr_nachhaltige_Entwicklung\\n\"Ziele für nachhaltige Entwicklung\") (SDG) der [Vereinten\\nNationen](/wiki/Vereinte_Nationen \"Vereinte Nationen\") (UN), [Corporate Social\\nResponsibility](/wiki/Corporate_Social_Responsibility \"Corporate Social\\nResponsibility\") (CSR), [Stakeholderkapitalismus](/wiki/Stakeholder\\n\"Stakeholder\"), [Risikomanagement](/wiki/Risikomanagement \"Risikomanagement\"),\\ndas [Vorsorgeprinzip](/wiki/Vorsorgeprinzip \"Vorsorgeprinzip\"), sowie die\\n[Grenzen des Wachstums](/wiki/Die_Grenzen_des_Wachstums \"Die Grenzen des\\nWachstums\").[10]\\n\\n## Rezeption\\n\\n[[Bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&veaction=edit&section=3 \"Abschnitt bearbeiten: Rezeption\") | [Quelltext bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&action=edit&section=3 \"Quellcode des Abschnitts bearbeiten: Rezeption\")]\\n\\nAnhänger hat die Bewegung insbesondere im Silicon Valley gefunden, wobei es zu\\neiner Spaltung zwischen den Anhängern von e/acc und den „Doomern“ gekommen\\nsein soll.[12] So soll es eine solche Spaltlinie auch bei dem KI-Unternehmen\\n[OpenAI](/wiki/OpenAI \"OpenAI\") geben.[13][14] Eine Reihe hochrangiger\\nPersönlichkeiten aus dem Silicon Valley, darunter die Investoren Marc\\nAndreessen und [Garry Tan](/w/index.php?title=Garry_Tan&action=edit&redlink=1\\n\"Garry Tan \\\\(Seite nicht vorhanden\\\\)\"), haben sich ausdrücklich für die\\nBewegung ausgesprochen, indem sie „e/acc“ zu ihren öffentlichen Profilen in\\nden sozialen Medien hinzugefügt haben.[8] Auch der Investor und Unternehmer\\n[Martin Shkreli](/wiki/Martin_Shkreli \"Martin Shkreli\") gilt als Anhänger.[15]\\n\\nAuf dem _Reagan National Defense Forum_ 2023 warnte US-Handelsministerin [Gina\\nRaimondo](/wiki/Gina_Raimondo \"Gina Raimondo\") davor, die mit der „effektiven\\nAkzeleration“ verbundene Mentalität des „move fast and break things“ zu\\nübernehmen. Sie betonte die Notwendigkeit, im Umgang mit KI Vorsicht walten zu\\nlassen.[1]\\n\\nIn den Medien wurde e/acc als „Kult“ und „Ideologie“ bezeichnet.[16][15]\\nTechnologieunternehmern wie Andreessen wurde vorgeworfen, mit der Förderung\\nvon e/acc vorwiegend eigene wirtschaftliche Interessen zu verfolgen und sich\\nin seinem „Techno-Optimistischen Manifest“ auf reaktionäre Denker wie Nick\\nLand und den italienischen [Protofaschisten](/wiki/Pr%C3%A4faschismus\\n\"Präfaschismus\") und [Futuristen](/wiki/Futurismus \"Futurismus\") [Filippo\\nTommaso Marinetti](/wiki/Filippo_Tommaso_Marinetti \"Filippo Tommaso\\nMarinetti\") zu beziehen.[6] Das _[Handelsblatt](/wiki/Handelsblatt\\n\"Handelsblatt\")_ charakterisierte dieses als Mischung aus „[Leni\\nRiefenstahl](/wiki/Leni_Riefenstahl \"Leni Riefenstahl\"), [George\\nOrwell](/wiki/George_Orwell \"George Orwell\") und [Arnold\\nSchwarzenegger](/wiki/Arnold_Schwarzenegger \"Arnold Schwarzenegger\")“.[16]\\n\\nDer [posthumanistischen](/wiki/Posthumanismus \"Posthumanismus\") Ideologie von\\ne/acc wurde vorgeworfen, gleichgültig gegenüber dem Schicksal der Menschheit\\nzu sein.[17]\\n\\n## Siehe auch\\n\\n[[Bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&veaction=edit&section=4 \"Abschnitt bearbeiten: Siehe auch\") | [Quelltext bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&action=edit&section=4 \"Quellcode des Abschnitts bearbeiten: Siehe auch\")]\\n\\n  * [Technologische Singularität](/wiki/Technologische_Singularit%C3%A4t \"Technologische Singularität\")\\n  * [Akzelerationismus](/wiki/Akzelerationismus \"Akzelerationismus\")\\n\\n## Weblinks\\n\\n[[Bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&veaction=edit&section=5 \"Abschnitt bearbeiten: Weblinks\") | [Quelltext bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&action=edit&section=5 \"Quellcode des Abschnitts bearbeiten: Weblinks\")]\\n\\n  * [Notes on e/acc principles and tenets](https://beff.substack.com/p/notes-on-eacc-principles-and-tenets) auf [Substack](/wiki/Substack \"Substack\")\\n  * [The Techno-Optimist Manifesto](https://a16z.com/the-techno-optimist-manifesto/) von [Marc Andreessen](/wiki/Marc_Andreessen \"Marc Andreessen\")\\n\\n## Einzelnachweise\\n\\n[[Bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&veaction=edit&section=6 \"Abschnitt bearbeiten: Einzelnachweise\") | [Quelltext bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&action=edit&section=6 \"Quellcode des Abschnitts bearbeiten: Einzelnachweise\")]\\n\\n  1. ↑ a b c [_This A.I. Subculture’s Motto: Go, Go, Go._](https://www.nytimes.com/2023/12/10/technology/ai-acceleration.html) In: _New York Times._ Abgerufen am 1. April 2024 (englisch).\\n  2. ↑ a b e/acc: [_Effective Accelerationism — e/acc._](https://effectiveaccelerationism.substack.com/p/repost-effective-accelerationism) In: _e/acc newsletter._ 31. Oktober 2022, abgerufen am 1. April 2024.\\n  3. ↑ Ali Breland: [_Meet the Silicon Valley CEOs who insist that greed is good._](https://www.motherjones.com/politics/2023/12/effective-accelerationism/) In: _Mother Jones._ Abgerufen am 1. April 2024 (amerikanisches Englisch).\\n  4. ↑ a b c d e Émile P. Torres: [_\\'Effective Accelerationism\\' and the Pursuit of Cosmic Utopia._](https://www.truthdig.com/articles/effective-accelerationism-and-the-pursuit-of-cosmic-utopia/) 14. Dezember 2023, abgerufen am 1. April 2024 (amerikanisches Englisch).\\n  5. ↑ a b c Beff Jezos, bayeslord: [_Notes on e/acc principles and tenets._](https://beff.substack.com/p/notes-on-eacc-principles-and-tenets) In: _Beff’s Newsletter._ 10. Juli 2022, abgerufen am 1. April 2024.\\n  6. ↑ a b Christian Stöcker: Effective Accelerationism: Die neue Silicon-Valley-Ideologie ist dunkel und kalt – Kolumne. In: Der Spiegel. 7. Januar 2024, [ISSN](/wiki/Internationale_Standardnummer_f%C3%BCr_fortlaufende_Sammelwerke \"Internationale Standardnummer für fortlaufende Sammelwerke\") [2195-1349](https://zdb-katalog.de/list.xhtml?t=iss%3D%222195-1349%22&key=cql) ([spiegel.de](https://www.spiegel.de/wissenschaft/mensch/effective-accelerationism-die-neue-silicon-valley-ideologie-ist-dunkel-und-kalt-kolumne-a-4b1422c3-0235-4b78-84fc-de2e289fc2f4) [abgerufen am 1. April 2024]).\\n  7. ↑ Elena Witzeck: Silicon-Valley-Ideologie des effektiven Akzelerationismus. In: FAZ.NET. 3. Februar 2024, [ISSN](/wiki/Internationale_Standardnummer_f%C3%BCr_fortlaufende_Sammelwerke \"Internationale Standardnummer für fortlaufende Sammelwerke\") [0174-4909](https://zdb-katalog.de/list.xhtml?t=iss%3D%220174-4909%22&key=cql) ([faz.net](https://www.faz.net/aktuell/feuilleton/debatten/silicon-valley-ideologie-des-effektiven-akzelerationismus-19492455.html) [abgerufen am 1. April 2024]).\\n  8. ↑ a b Hasan Chowdhury: [_Get the lowdown on \\'e/acc\\' — Silicon Valley\\'s favorite obscure theory about progress at all costs, which has been embraced by Marc Andreessen._](https://www.businessinsider.com/silicon-valley-tech-leaders-accelerationism-eacc-twitter-profiles-2023-7) Abgerufen am 1. April 2024 (amerikanisches Englisch).\\n  9. ↑ Emily Baker-White: [_Who Is @BasedBeffJezos, The Leader Of The Tech Elite’s ‘E/Acc’ Movement?_](https://www.forbes.com/sites/emilybaker-white/2023/12/01/who-is-basedbeffjezos-the-leader-of-effective-accelerationism-eacc/) Abgerufen am 1. April 2024 (englisch).\\n  10. ↑ a b Marc Andreessen: [_The Techno-Optimist Manifesto._](https://a16z.com/the-techno-optimist-manifesto/) 16. Oktober 2023, abgerufen am 1. April 2024 (englisch).\\n  11. ↑ [_Marc Andreessen just dropped a ‘Techno-Optimist Manifesto’ that sees a world of 50 billion people settling other planets._](https://fortune.com/2023/10/16/marc-andreessen-techno-optimist-manifesto-ai-50-billion-people-billionaire-vc/) Abgerufen am 1. April 2024 (englisch).\\n  12. ↑ A Cultural Divide Over AI Forms in Silicon Valley. In: Bloomberg.com. 6. Dezember 2023 ([bloomberg.com](https://www.bloomberg.com/news/newsletters/2023-12-06/effective-accelerationism-and-beff-jezos-form-new-tech-tribe) [abgerufen am 1. April 2024]).\\n  13. ↑ [_Effective Accelerationism: Die Philosophie hinter dem Open-AI-Drama verhindert eine ethische KI - HORIZONT._](https://www.horizont.net/tech/nachrichten/the-next-step-effective-accelerationism-warum-die-philosophie-hinter-dem-open-ai-drama-eine-ethische-ki-verhindert-216144) Abgerufen am 1. April 2024.\\n  14. ↑ Clemens Römer: [_KI-Entwicklung bei OpenAI: Effective Altruism vs. Effective Accelerationism - Netzpiloten.de._](https://www.netzpiloten.de/ki-entwicklung-bei-openai-effective-altruism-vs-effective-accelerationism/) In: _Netzpiloten Magazin._ 14. Dezember 2023, abgerufen am 1. April 2024 (deutsch).\\n  15. ↑ a b [_\\'It\\'s a Cult\\': Inside Effective Accelerationism._](https://www.theinformation.com/articles/its-a-cult-inside-effective-accelerationism-the-pro-ai-movement-taking-over-silicon-valley) In: _The Information._ Abgerufen am 1. April 2024.\\n  16. ↑ a b [_Silicon Scientology: Wenn KI zur Ideologie wird._](https://www.handelsblatt.com/meinung/kolumnen/kolumne-kreative-zerstoerung-silicon-scientology-wenn-ki-zur-ideologie-wird/29462864.html) In: _Handelsblatt._ Abgerufen am 1. April 2024.\\n  17. ↑ Katherine Tangalakis-Lippert, Hannah Getahun: [_The \\'Effective Accelerationism\\' movement doesn\\'t care if humans are replaced by AI as long as they\\'re there to make money from it._](https://www.businessinsider.com/effective-accelerationism-humans-replaced-by-ai-2023-12) Abgerufen am 1. April 2024 (amerikanisches Englisch).\\n\\n![](https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?useformat=desktop&type=1x1&usesul3=0)\\n\\nAbgerufen von\\n„[https://de.wikipedia.org/w/index.php?title=Effektiver_Akzelerationismus&oldid=253581918](https://de.wikipedia.org/w/index.php?title=Effektiver_Akzelerationismus&oldid=253581918)“\\n\\n[Kategorien](/wiki/Wikipedia:Kategorien \"Wikipedia:Kategorien\"):\\n\\n  * [Philosophische Strömung](/wiki/Kategorie:Philosophische_Str%C3%B6mung \"Kategorie:Philosophische Strömung\")\\n  * [Künstliche Intelligenz](/wiki/Kategorie:K%C3%BCnstliche_Intelligenz \"Kategorie:Künstliche Intelligenz\")\\n  * [Philosophie der Gegenwart](/wiki/Kategorie:Philosophie_der_Gegenwart \"Kategorie:Philosophie der Gegenwart\")\\n\\n## Navigationsmenü\\n\\n###  Meine Werkzeuge\\n\\n  * Nicht angemeldet\\n  * [Diskussionsseite](/wiki/Spezial:Meine_Diskussionsseite \"Diskussion über Änderungen von dieser IP-Adresse \\\\[n\\\\]\")\\n  * [Beiträge](/wiki/Spezial:Meine_Beitr%C3%A4ge \"Eine Liste der Bearbeitungen, die von dieser IP-Adresse gemacht wurden \\\\[y\\\\]\")\\n  * [Benutzerkonto erstellen](/w/index.php?title=Spezial:Benutzerkonto_anlegen&returnto=Effektiver+Akzelerationismus \"Wir ermutigen dich dazu, ein Benutzerkonto zu erstellen und dich anzumelden. Es ist jedoch nicht zwingend erforderlich.\")\\n  * [Anmelden](/w/index.php?title=Spezial:Anmelden&returnto=Effektiver+Akzelerationismus \"Anmelden ist zwar keine Pflicht, wird aber gerne gesehen. \\\\[o\\\\]\")\\n\\n###  Namensräume\\n\\n  * [Artikel](/wiki/Effektiver_Akzelerationismus \"Seiteninhalt anzeigen \\\\[c\\\\]\")\\n  * [Diskussion](/w/index.php?title=Diskussion:Effektiver_Akzelerationismus&action=edit&redlink=1 \"Diskussion zum Seiteninhalt \\\\(Seite nicht vorhanden\\\\) \\\\[t\\\\]\")\\n\\nDeutsch\\n\\n###  Ansichten\\n\\n  * [Lesen](/wiki/Effektiver_Akzelerationismus)\\n  * [Bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&veaction=edit \"Diese Seite mit dem VisualEditor bearbeiten \\\\[v\\\\]\")\\n  * [Quelltext bearbeiten](/w/index.php?title=Effektiver_Akzelerationismus&action=edit \"Den Quelltext dieser Seite bearbeiten \\\\[e\\\\]\")\\n  * [Versionsgeschichte](/w/index.php?title=Effektiver_Akzelerationismus&action=history \"Frühere Versionen dieser Seite \\\\[h\\\\]\")\\n\\nWeitere\\n\\n### Suche\\n\\n[](/wiki/Wikipedia:Hauptseite \"Hauptseite\")\\n\\n###  Navigation\\n\\n  * [Hauptseite](/wiki/Wikipedia:Hauptseite \"Hauptseite besuchen \\\\[z\\\\]\")\\n  * [Themenportale](/wiki/Portal:Wikipedia_nach_Themen)\\n  * [Zufälliger Artikel](/wiki/Spezial:Zuf%C3%A4llige_Seite \"Zufällige Seite aufrufen \\\\[x\\\\]\")\\n  * [Spezialseiten](/wiki/Spezial:Spezialseiten)\\n\\n###  Mitmachen\\n\\n  * [Artikel verbessern](/wiki/Wikipedia:Beteiligen)\\n  * [Neuen Artikel anlegen](/wiki/Hilfe:Neuen_Artikel_anlegen)\\n  * [Autorenportal](/wiki/Wikipedia:Autorenportal \"Info-Zentrum über Beteiligungsmöglichkeiten\")\\n  * [Hilfe](/wiki/Hilfe:%C3%9Cbersicht \"Übersicht über Hilfeseiten\")\\n  * [Letzte Änderungen](/wiki/Spezial:Letzte_%C3%84nderungen \"Liste der letzten Änderungen in Wikipedia \\\\[r\\\\]\")\\n  * [Kontakt](/wiki/Wikipedia:Kontakt \"Kontaktmöglichkeiten\")\\n  * [Spenden](https://donate.wikimedia.org/?wmf_source=donate&wmf_medium=sidebar&wmf_campaign=de.wikipedia.org&uselang=de \"Unterstütze uns\")\\n\\n###  Werkzeuge\\n\\n  * [Links auf diese Seite](/wiki/Spezial:Linkliste/Effektiver_Akzelerationismus \"Liste aller Seiten, die hierher verlinken \\\\[j\\\\]\")\\n  * [Änderungen an verlinkten Seiten](/wiki/Spezial:%C3%84nderungen_an_verlinkten_Seiten/Effektiver_Akzelerationismus \"Letzte Änderungen an Seiten, die von hier verlinkt sind \\\\[k\\\\]\")\\n  * [Permanenter Link](/w/index.php?title=Effektiver_Akzelerationismus&oldid=253581918 \"Dauerhafter Link zu dieser Seitenversion\")\\n  * [Seiten\\xad\\xadinformationen](/w/index.php?title=Effektiver_Akzelerationismus&action=info \"Weitere Informationen über diese Seite\")\\n  * [Artikel zitieren](/w/index.php?title=Spezial:Zitierhilfe&page=Effektiver_Akzelerationismus&id=253581918&wpFormIdentifier=titleform \"Hinweise, wie diese Seite zitiert werden kann\")\\n  * [Kurzlink](/w/index.php?title=Spezial:URL-K%C3%BCrzung&url=https%3A%2F%2Fde.wikipedia.org%2Fwiki%2FEffektiver_Akzelerationismus)\\n  * [QR-Code herunterladen](/w/index.php?title=Spezial:QrCode&url=https%3A%2F%2Fde.wikipedia.org%2Fwiki%2FEffektiver_Akzelerationismus)\\n\\n###  Drucken/\\u200bexportieren\\n\\n  * [Als PDF herunterladen](/w/index.php?title=Spezial:DownloadAsPdf&page=Effektiver_Akzelerationismus&action=show-download-screen)\\n  * [Druckversion](/w/index.php?title=Effektiver_Akzelerationismus&printable=yes \"Druckansicht dieser Seite \\\\[p\\\\]\")\\n\\n###  In anderen Projekten\\n\\n  * [Wikidata-Datenobjekt](https://www.wikidata.org/wiki/Special:EntityPage/Q123509272 \"Link zum verbundenen Objekt im Datenrepositorium \\\\[g\\\\]\")\\n\\n###  In anderen Sprachen\\n\\n  * [العربية](https://ar.wikipedia.org/wiki/%D8%AA%D8%B3%D8%B1%D9%8A%D8%B9%D9%8A%D8%A9_%D9%81%D8%B9%D8%A7%D9%84%D8%A9 \"تسريعية فعالة – Arabisch\")\\n  * [English](https://en.wikipedia.org/wiki/Effective_accelerationism \"Effective accelerationism – Englisch\")\\n  * [Español](https://es.wikipedia.org/wiki/Aceleracionismo_eficaz \"Aceleracionismo eficaz – Spanisch\")\\n  * [Français](https://fr.wikipedia.org/wiki/Acc%C3%A9l%C3%A9rationnisme_efficace \"Accélérationnisme efficace – Französisch\")\\n  * [日本語](https://ja.wikipedia.org/wiki/%E5%8A%B9%E6%9E%9C%E7%9A%84%E5%8A%A0%E9%80%9F%E4%B8%BB%E7%BE%A9 \"効果的加速主義 – Japanisch\")\\n  * [한국어](https://ko.wikipedia.org/wiki/%ED%9A%A8%EA%B3%BC%EC%A0%81_%EA%B0%80%EC%86%8D%EC%A3%BC%EC%9D%98 \"효과적 가속주의 – Koreanisch\")\\n  * [Türkçe](https://tr.wikipedia.org/wiki/Etkili_ivmecilik \"Etkili ivmecilik – Türkisch\")\\n  * [中文](https://zh.wikipedia.org/wiki/%E6%9C%89%E6%95%88%E5%8A%A0%E9%80%9F%E4%B8%BB%E7%BE%A9 \"有效加速主義 – Chinesisch\")\\n\\n[Links\\nbearbeiten](https://www.wikidata.org/wiki/Special:EntityPage/Q123509272#sitelinks-\\nwikipedia \"Links auf Artikel in anderen Sprachen bearbeiten\")\\n\\n  * Diese Seite wurde zuletzt am 22. Februar 2025 um 19:15 Uhr bearbeitet.\\n  * [Abrufstatistik](https://pageviews.wmcloud.org/?pages=Effektiver_Akzelerationismus&project=de.wikipedia.org) · [Autoren](https://xtools.wmcloud.org/authorship/de.wikipedia.org/Effektiver_Akzelerationismus?uselang=de)\\n\\n  \\n\\nDer Text ist unter der Lizenz [„Creative-Commons Namensnennung – Weitergabe\\nunter gleichen Bedingungen“](https://creativecommons.org/licenses/by-\\nsa/4.0/deed.de) verfügbar; Informationen zu den Urhebern und zum Lizenzstatus\\neingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall\\ndurch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die\\nInhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website\\nerklären Sie sich mit den\\n[Nutzungsbedingungen](https://foundation.wikimedia.org/wiki/Policy:Terms_of_Use/de)\\nund der\\n[Datenschutzrichtlinie](https://foundation.wikimedia.org/wiki/Policy:Privacy_policy/de)\\neinverstanden.  \\n\\nWikipedia® ist eine eingetragene Marke der Wikimedia Foundation Inc.\\n\\n  * [Datenschutz](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy/de)\\n  * [Über Wikipedia](/wiki/Wikipedia:%C3%9Cber_Wikipedia)\\n  * [Impressum](/wiki/Wikipedia:Impressum)\\n  * [Verhaltenskodex](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct)\\n  * [Entwickler](https://developer.wikimedia.org)\\n  * [Statistiken](https://stats.wikimedia.org/#/de.wikipedia.org)\\n  * [Stellungnahme zu Cookies](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement)\\n  * [Mobile Ansicht](//de.m.wikipedia.org/w/index.php?title=Effektiver_Akzelerationismus&mobileaction=toggle_view_mobile)\\n\\n  * [![Wikimedia Foundation](/static/images/footer/wikimedia.svg)](https://wikimediafoundation.org/)\\n  * [![Powered by MediaWiki](/w/resources/assets/mediawiki_compact.svg)](https://www.mediawiki.org/)\\n\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lese die Webseite aus und gib den print Befehl den Text als Output\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://de.wikipedia.org/wiki/Effektiver_Akzelerationismus\"]\n",
    ")\n",
    "\n",
    "display(Markdown(f\"{documents}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be4f154",
   "metadata": {},
   "source": [
    "# Kapitel 2: Tools\n",
    "\n",
    "Ein weiteres grundlegendes Feature in Llamaindex ist die Möglichkeit, ein Tool zu definieren und dem Sprachmodell Zugriff darauf zu gewähren.\n",
    "\n",
    "Eine der aktuellen Herausforderungen moderner Sprachmodelle besteht darin, dass sie oft Schwierigkeiten mit Arithmetik haben. Dieses Problem können wir lösen, indem wir dem Sprachmodell einen Taschenrechner zur Verfügung stellen, den es bei Bedarf nutzen kann.\n",
    "\n",
    "Um es einfach zu halten, erstellen wir zunächst eine Python-Funktion `taschenrechner(a: int, b: int) -> int`, die zwei Integer-Werte annimmt und die Summe davon ausgibt.\n",
    "\n",
    "Anschließend erstellen wir ein `FunctionTool`-Objekt, dem wir diese Funktion übergeben, damit das Sprachmodell darauf zugreifen kann, wenn es die Funktion benötigt.\n",
    "\n",
    "Dokumentation und Codebeispiel zu `FunctionTool`: [https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af3e642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hier ein das FunctionTool taschenrechner definieren\n",
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def taschenrechner(a: int, b: int) -> int: #explizite Angabe der Typen und des Returntypes wichtig für den Prompt\n",
    "    \"\"\"Useful for add two integer numbers.\"\"\" #wichtig da es als Prompt übergeben wird\n",
    "    return int(a)+int(b) #cast darf nicht vergessen werden, ansonsten einfach string übergeben\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    taschenrechner\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff5912",
   "metadata": {},
   "source": [
    "Um das FunctionTool für das Sprachmodell nutzbar zu machen, definieren wir zunächst ein Ollama LLM innerhalb von LlamaIndex. Stelle sicher, dass der Ollama server läuft.\n",
    "\n",
    "LlamaIndex Ollama-Dokumentation: [https://docs.llamaindex.ai/en/stable/api_reference/llms/ollama/](https://docs.llamaindex.ai/en/stable/api_reference/llms/ollama/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14045066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called scattering, which occurs when sunlight interacts with the tiny molecules of gases in the Earth's atmosphere.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. When sunlight enters the Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2).\n",
      "2. These molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths.\n",
      "3. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century.\n",
      "4. As a result of this scattering, the blue light is dispersed in all directions and reaches our eyes from all parts of the sky.\n",
      "5. Our brains interpret this scattered light as the color blue, which is why the sky appears blue during the daytime.\n",
      "\n",
      "The reason we don't see the sky as blue at sunrise or sunset is because the sunlight has to travel through more of the Earth's atmosphere to reach us, which scatters the shorter wavelengths even more. This is known as Mie scattering, named after the German physicist Gustav Mie, who first described it in the early 20th century.\n",
      "\n",
      "During sunrise and sunset, the light has to travel through more of the atmosphere, which scatters the blue light even more, leaving mainly longer wavelengths (like red and orange) to reach our eyes. That's why the sky often appears more reddish or orange during these times.\n",
      "\n",
      "So, in summary, the sky appears blue because of the scattering of sunlight by tiny molecules in the Earth's atmosphere, with shorter wavelengths being scattered more than longer wavelengths.\n"
     ]
    }
   ],
   "source": [
    "#hier ein Ollama LLM definieren\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=60.0)\n",
    "\n",
    "\n",
    "\n",
    "#frage das LLM warum der Himmel blau ist\n",
    "response = llm.complete(\"Why is the sky blue?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bdcaae",
   "metadata": {},
   "source": [
    "Da wir nun sowohl ein Sprachmodell als auch ein Tool haben, können wir mit der eingebauten LLM-Funktion `.predict_and_call()` arbeiten. Diese ermöglicht es uns, einen sogenannten ReAct-Agent zu definieren und ihm eine Liste von Tools zu übergeben – in unserem Fall nur ein Tool.\n",
    "\n",
    "Die Aufgabe ist nun, das Modell zu fragen, was die Summe von 12345 und 30075 ist.\n",
    "\n",
    "Codebeispiel: [https://medium.com/@samad19472002/agentic-rag-application-using-llamaindex-tool-calling-30bfef6cb4fb](https://medium.com/@samad19472002/agentic-rag-application-using-llamaindex-tool-calling-30bfef6cb4fb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17c4556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: taschenrechner with args: {\"a\": \"12345\", \"b\": \"30075\"}\n",
      "=== Function Output ===\n",
      "42420\n",
      "42420\n"
     ]
    }
   ],
   "source": [
    "#Frage hier das Sprachmodell was die Summe von 12345 und 30075 ist\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=60.0)\n",
    "response = llm.predict_and_call(\n",
    "    [tool],\n",
    "    \"Tell me what ist the sum of 12345 added with 30075\",\n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d12833b",
   "metadata": {},
   "source": [
    "Versuche nun, die Funktion `taschenrechner` so umzuschreiben, dass sie Addition, Subtraktion, Multiplikation und Division beherrscht. Zudem soll sie Float-Werte annehmen und einen Float-Wert zurückgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2706468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hier die neue Taschenrechner Funktion umschreiben\n",
    "\n",
    "#Problem war dass man gedacht hat llm kann if else nicht erkennen ohne dass immer ein extra prompt notwendig wäre\n",
    "\n",
    "def taschenrechner_advanced(a: float, b: float, operation: str) -> float:\n",
    "    \"\"\"Perform basic arithmetic operations.\n",
    "    when you chooose operation \"+\":\n",
    "    ....\n",
    "    when you...\n",
    "    \n",
    "    \"\"\"\n",
    "    if operation == \"+\":\n",
    "        return float(a) + float(b)\n",
    "    elif operation == \"-\":\n",
    "        return float(a) - float(b)\n",
    "    elif operation == \"*\":\n",
    "        return float(a) * float(b)\n",
    "    elif operation == \"/\":\n",
    "        return float(a) / float(b) \n",
    "    else:\n",
    "        raise ValueError(\"Invalid operation\")\n",
    "\n",
    "tool_advanced = FunctionTool.from_defaults(\n",
    "    taschenrechner_advanced,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f9dad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: taschenrechner_advanced with args: {\"a\": \"10045\", \"b\": \"6522455\", \"operation\": \"-\"}\n",
      "=== Function Output ===\n",
      "-6512410.0\n",
      "-6512410.0\n"
     ]
    }
   ],
   "source": [
    "#Frage das LLM was 10045 - 6522455 gibt\n",
    "llm = Ollama(model=\"llama3.2\", request_timeout=60.0)\n",
    "response = llm.predict_and_call(\n",
    "    [tool_advanced],\n",
    "    \"Was ist 10045 subtrahiert mit 6522455\",\n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f023ac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: taschenrechner_advanced with args: {\"a\": \"42\", \"b\": \"54695\", \"operation\": \"*\"}\n",
      "=== Function Output ===\n",
      "2297190.0\n",
      "2297190.0\n"
     ]
    }
   ],
   "source": [
    "#Frage das LLM was 42 * 54695 gibt\n",
    "response = llm.predict_and_call(\n",
    "    [tool_advanced],\n",
    "    \"Was ist 42 multiplied with mit 54695\",\n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b2e212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: taschenrechner_advanced with args: {\"a\": \"42\", \"b\": \"54695\", \"operation\": \"/\"}\n",
      "=== Function Output ===\n",
      "0.000767894688728403\n",
      "0.000767894688728403\n"
     ]
    }
   ],
   "source": [
    "#Frage das LLM was 42 / 54695 gibt\n",
    "response = llm.predict_and_call(\n",
    "    [tool_advanced],\n",
    "    \"Was ist 42 / 54695\",\n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970abaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: taschenrechner_advanced with args: {\"a\": \"9\", \"b\": \"0\", \"operation\": \"/\"}\n",
      "=== Function Output ===\n",
      "Encountered error: float division by zero\n",
      "Encountered error: float division by zero\n"
     ]
    }
   ],
   "source": [
    "#Frage das LLM was 9 / 0 gibt\n",
    "response = llm.predict_and_call(\n",
    "    [tool_advanced],\n",
    "    \"Was ist 9 / 0\",\n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054a87f",
   "metadata": {},
   "source": [
    "Schreibe die Funktion `taschenrechner` wieder so um, dass es Nulldivisonen abfängt und eine Sinnvolle Antwort, z.B. \"Halt stop! Nulldivision!\" ausgibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "304c7cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: taschenrechner_advanced with args: {\"a\": \"9\", \"b\": \"0\", \"operation\": \"/\"}\n",
      "=== Function Output ===\n",
      "Halt Stop! Nicht durch 0 dividieren!\n",
      "Halt Stop! Nicht durch 0 dividieren!\n"
     ]
    }
   ],
   "source": [
    "#hier die neue Taschenrechner Funktion umschreiben\n",
    "def taschenrechner_advanced(a: float, b: float, operation: str) -> float:\n",
    "    \"\"\"Perform basic arithmetic operations.\"\"\"\n",
    "    if operation == \"+\":\n",
    "        return float(a) + float(b)\n",
    "    elif operation == \"-\":\n",
    "        return float(a) - float(b)\n",
    "    elif operation == \"*\":\n",
    "        return float(a) * float(b)\n",
    "    elif operation == \"/\":\n",
    "        if float(b) == 0:\n",
    "            return \"Halt Stop! Nicht durch 0 dividieren!\"\n",
    "        else:\n",
    "            return float(a) / float(b) \n",
    "    else:\n",
    "        raise ValueError(\"Invalid operation\")\n",
    "\n",
    "tool_advanced = FunctionTool.from_defaults(\n",
    "    taschenrechner_advanced,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#frage wieder das LLM was 9 / 0 gibt\n",
    "response = llm.predict_and_call(\n",
    "    [tool_advanced],\n",
    "    \"Was ist 9 / 0\",\n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193081d",
   "metadata": {},
   "source": [
    "# Kapitel 3: Vektor Datenbank\n",
    "\n",
    "Im ersten Kapitel haben wir gelernt, wie man in LlamaIndex Daten auslesen kann. Jetzt möchten wir einen Schritt weiter gehen und das Sprachmodell mit diesen Daten arbeiten lassen. \n",
    "\n",
    "Wir könnten die Daten direkt in unseren Prompt einfügen, allerdings kommt es häufig vor, dass die Datenmenge zu groß ist und das Kontextfenster überschritten wird. Um dieses Problem zu lösen, verwenden wir eine spezielle Datenbank, um unsere Textdateien dort abzulegen. Danach können wir durch semantische Ähnlichkeit herausfinden, welche Teile der Daten unserer Anfrage am nächsten kommen und diese ausgeben.\n",
    "\n",
    "Dazu lesen wir erneut den Ordner `data/` aus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "140792bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hier wie in Kapitel 1 den Ordner \"data/\" einlesen\n",
    "reader = SimpleDirectoryReader(input_dir=\"data/\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad744bf",
   "metadata": {},
   "source": [
    "Wir möchten nun eine Datenbank definieren, die uns hilft, schneller die passenden Teile unserer Daten zu finden. Dafür benötigen wir ein sogenanntes Embedding-Modell.\n",
    "\n",
    "Öffne das Terminal und überprüfe, ob das Modell `mxbai-embed-large` verfügbar ist.\n",
    "\n",
    "Falls nicht, kann das Modell mit folgendem Befehl heruntergeladen werden:\n",
    "\n",
    "```bash\n",
    "ollama pull mxbai-embed-large\n",
    "```\n",
    "\n",
    "Ist das Modell verfügbar, muss es als Standard-Embedding-Modell festgelegt werden.\n",
    "Und auch das normale Sprachmodell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c367d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fertig\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large\")\n",
    "Settings.llm = Ollama(model=\"llama3.2\") # hier das Ollama LLM LLM als Standart definieren\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d539faa",
   "metadata": {},
   "source": [
    "Erstellen wir nun ein `VectorStoreIndex`-Objekt, um die Daten effizient zu verwalten und das Sprachmodell auf relevante Informationen zugreifen zu lassen.\n",
    "\n",
    "Dokumentation: [VectorStoreIndex](https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3e6e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x0000020CC0BB7860>\n",
      "fertig\n"
     ]
    }
   ],
   "source": [
    "# Erstelle ein VectorStoreIndex Objekt\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load documents and build index\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"data\"\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "print(index)\n",
    "#print(documents)\n",
    "print(\"fertig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169eb578",
   "metadata": {},
   "source": [
    "Da wir nun eine Vektordatenbank haben, können wir einen `VectorIndexRetriever` definieren, der uns ermöglicht, die Datenbank direkt abzufragen.\n",
    "\n",
    "In der Datenbank sind die Daten in sogenannte Chunks oder Schnipsel aufgeteilt. Bei einer Abfrage werden diejenigen Schnipsel ausgewählt, die semantisch am besten zur Anfrage passen (weitere Informationen zur Vektor-Similaritätssuche findest du hier: [Vector Similarity Search](https://medium.com/@serkan_ozal/vector-similarity-search-53ed42b951d9)). Die obersten \\(k\\)-Treffer, z.B. die Top 5 Schnipsel, werden dann ausgegeben.\n",
    "\n",
    "Um eine solche Anfrage auszuführen, erstelle zunächst einen Retriever, indem du im `VectorStoreIndex`-Objekt die Funktion `.as_retriever(choice_batch_size)` aufrufst und den Parameter `choice_batch_size` auf 5 setzt.\n",
    "\n",
    "Jetzt kann man dem erstellten Retriever eine Anfrage senden. Nutze dabei die Funktion `.retrieve(query)`, wobei \n",
    "`query` ein String ist, der eine Anfrage in natürlicher Sprache enthält. Suche beispielsweise mal danach, was ein Kalman Filter ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14d8417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NodeWithScore(node=TextNode(id_='b3fd40cd-7004-49f0-ace3-7b92bc25a265', embedding=None, metadata={'page_label': '72', 'file_name': 'Datenfusion.pdf', 'file_path': 'Z:\\\\KI AG\\\\data\\\\Datenfusion.pdf', 'file_type': 'application/pdf', 'file_size': 2810997, 'creation_date': '2025-03-03', 'last_modified_date': '2025-03-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b65e7e28-b76c-46c0-9ec3-dc090e4d2c46', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '72', 'file_name': 'Datenfusion.pdf', 'file_path': 'Z:\\\\KI AG\\\\data\\\\Datenfusion.pdf', 'file_type': 'application/pdf', 'file_size': 2810997, 'creation_date': '2025-03-03', 'last_modified_date': '2025-03-03'}, hash='2e4c8ee92cd912ec6c9ad5fa22e93b9ca850c9548edecae18bc47398a3a96405')}, text='Vorlesung Datenfusion Kap. 7 © 2024 Prof. Dr. R. Marchthaler\\nKalman-Filter Gleichungen\\nMit den GrößenAd = 1, Bd = 0, C = 1, D = 0 und Gd = 1 und der Annahme, dass sich\\ndie Varianz des SystemrauschensQund die Varianz des MessrauschensRnicht über die Zeit\\nverändert, folgt:\\nK(k) = ˆP(k) ·CT ·\\n(\\nC·ˆP(k) ·CT + R(k)\\n)−1\\n= ˆP(k) ·\\n(ˆP(k) + R\\n)−1\\n(58)\\n˜x(k) = ˆ x(k) + K(k) ·\\n(\\ny(k) −C·ˆx(k) −D·u(k)\\n)\\n= ˆx(k) + K(k) ·\\n(\\ny(k) −ˆx(k)\\n)\\n(59)\\n˜P(k) =\\n(\\nI−K(k) ·C\\n)\\n·ˆP(k) =\\n(\\n1 −K(k)\\n)\\n·ˆP(k) (60)\\nˆx(k+ 1) = Ad ·˜x(k) + Bd ·u(k) = ˜x(k) (61)\\nˆP(k+ 1) = Ad ·˜P(k) ·AT\\nd + Gd ·Q(k) ·GT\\nd = ˜P(k) + Q (62)\\nSetzt man Gleichung (59) in Gleichung (61) und Gleichung (60) in Gleichung (62) ein, reduziert\\nsich die Berechnung auf drei Gleichungen:\\nK(k) = ˆP(k) ·\\n(ˆP(k) + R\\n)−1\\n(63)\\nˆx(k+ 1) = ˆx(k) + K(k) ·\\n(\\ny(k) −ˆx(k)\\n)\\n(64)\\nˆP(k+ 1) =\\n(\\n1 −K(k)\\n)\\n·ˆP(k) + Q (65)\\nMit dem Startwert:ˆx(1) = y(1) und ˆP(1) = 100·Gd·Q·Gd = 100·Q Folie 4.85\\nMatlab-Code\\nload -ascii data_k_t_y_z_a_v.dat;\\ny = data_k_t_y_z_a_v(:,3);\\n%%% INITIALISIERUNG KALMAN-FILTER %%%\\nR = 0.0053;\\nDelta = 0.1;\\nQ = 0.01*(Delta^2);\\nx(1) = y(1);\\nP(1) = 100*Q;\\n%%% ZYKLISCHE BERECHNUNG KALMAN-FILTER %%%\\nfor k=1:length(y)\\nK(k) = P(k)/(P(k) + R);\\nx(k+1) = x(k) + K(k)*(y(k) - x(k));\\nP(k+1) = (1 - K(k))*P(k) + Q;\\nend\\nFolie 4.86\\nGeschätzte Zustandsgröße˜x\\n80 100 120 140 160 180\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nk\\ngemessenes Signaly(k)\\n˜x(k) mitQ= 0.12\\n100\\n˜x(k) mitQ= 0.022\\n100\\n˜x(k) mitQ= 0.52\\n100\\nFolie 4.87\\n-72-', mimetype='text/plain', start_char_idx=0, end_char_idx=1449, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.851934317441899), NodeWithScore(node=TextNode(id_='a42813ec-0ba3-4cc1-89af-a424117dc29e', embedding=None, metadata={'page_label': '2', 'file_name': 'Datenfusion.pdf', 'file_path': 'Z:\\\\KI AG\\\\data\\\\Datenfusion.pdf', 'file_type': 'application/pdf', 'file_size': 2810997, 'creation_date': '2025-03-03', 'last_modified_date': '2025-03-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a6a26e60-f2e2-49e6-9f06-2829b4079e9d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': 'Datenfusion.pdf', 'file_path': 'Z:\\\\KI AG\\\\data\\\\Datenfusion.pdf', 'file_type': 'application/pdf', 'file_size': 2810997, 'creation_date': '2025-03-03', 'last_modified_date': '2025-03-03'}, hash='80d5cbb6ad602724f2b18b229753ddaa7585f574955bcae6550aa0628c06ccc6'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='74b66a7c-1f17-48c6-adc8-705f409f8f1b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4b38c30922d152fb6601364bb8c9a95f1e391e91cde1559d7d210bad82481bfd')}, text='Vorlesung Datenfusion Inhaltsverzeichnis © 2024 Prof. Dr. R. Marchthaler\\nIII Datenfusion zeitinvarianter Größen 29\\n5 Datenfusion zeitinvariante Größen 29\\n5.1 Schätzung Skalar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n5.2 Schätzung Vektor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5.3 Gewichtete Schätzungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n5.4 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\nIV Kalman-Filter 40\\n6 Zeitvariante Größen 40\\n6.1 Zustandsraum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n6.2 Systemeigenschaften . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n6.3 Zeitdiskrete Systeme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\n6.4 System- und Messrauschen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n6.5 Lernziele . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\n7 Klassisches Kalman-Filter 54\\n7.1 Herleitung Filtergleichungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n7.2 Beispiel 1: Beta-Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n7.3 Kochrezept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', mimetype='text/plain', start_char_idx=0, end_char_idx=1330, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8475862934007615)]\n"
     ]
    }
   ],
   "source": [
    "#Erstelle ein VectorIndexRetriever Objekt\n",
    "VectorindexRetriever= index.as_retriever(choice_batch_size=5) \n",
    "#leichtsinnsfehler anstatt Objekt index die Klasse VectorstoreIndex gewählt\n",
    "\n",
    "#Frage den Retriever nach dem \"Kalman Filter\" und gib das Ergebnis sinnvoll mit einer print-Schleife aus\n",
    "response= VectorindexRetriever.retrieve(\"Was ist ein Kalman Filter?\")\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909aabb2",
   "metadata": {},
   "source": [
    "Jetzt möchten wir nicht nur die Datenbank abfragen, sondern auch die Ergebnisse dem Sprachmodell übergeben.  \n",
    "Hierfür möchten wir, wie im Kapitel 1, einen Page Reader verwenden, dieses Mal jedoch den `BeautifulSoupWebReader`.\n",
    "\n",
    "Lade damit die folgende Webseite: [https://arxiv.org/html/2410.22352v1](https://arxiv.org/html/2410.22352v1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c3d4152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='https://arxiv.org/html/2410.22352v1', embedding=None, metadata={'URL': 'https://arxiv.org/html/2410.22352v1'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"\\n\\n\\n\\nNeuromorphic Programming: Emerging Directions for Brain-Inspired Hardware\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nI Introduction\\n\\nII Motivation\\n\\nDomain\\nPlasticity\\nStochasticity\\nDecentralization\\nUnobservability\\n\\n\\n\\nIII Theoretical framework\\n\\nIII-A Computing with physical systems\\nIII-B Computer programs\\nIII-C Programming process\\nIII-D Languages and Paradigms\\n\\n\\n\\nIV Programming Paradigms\\n\\nIV-A Imperative programming\\nIV-B Declarative\\nIV-C Decentralized programming\\nIV-D Automated programming\\nIV-E Non-digital programming\\n\\n\\n\\nV Neuromorphic Programming\\n\\nNeuromorphic co-design\\nMachine learning methods\\nOnline learning\\nEvolutionary methods\\nSpiking neuromorphic algorithms\\nNeurocomputational primitives\\nHigher abstractions\\n\\n\\nVI Future approaches to programming brain-inspired hardware\\n\\n\\n\\n\\n\\nNeuromorphic Programming: Emerging Directions for Brain-Inspired Hardware\\n\\n\\nSteven Abreu\\n\\nCogniGron Center & Bernoulli Institute\\nUniversity of Groningen\\nGroningen, Netherlands \\ns.abreu@rug.nl\\n\\n\\u2003\\u2003\\nJens E. Pedersen\\n\\nComputational Science and Technology\\nKTH Royal Institute of Technology\\nStockholm, Sweden \\njeped@kth.se\\n\\n\\n\\nAbstract\\nThe value of brain-inspired neuromorphic computers critically depends on our ability to program them for relevant tasks. Currently, neuromorphic hardware often relies on machine learning methods adapted from deep learning.\\nHowever, neuromorphic computers have potential far beyond deep learning if we can only harness their energy efficiency and full computational power.\\nNeuromorphic programming will necessarily be different from conventional programming, requiring a paradigm shift in how we think about programming.\\nThis paper presents a conceptual analysis of programming within the context of neuromorphic computing, challenging conventional paradigms and proposing a framework that aligns more closely with the physical intricacies of these systems.\\nOur analysis revolves around five characteristics that are fundamental to neuromorphic programming and provides a basis for comparison to contemporary programming methods and languages.\\nBy studying past approaches, we contribute a framework that advocates for underutilized techniques and calls for richer abstractions to effectively instrument the new hardware class.\\n\\n\\n\\n\\nIndex Terms: \\nneuromorphic computing, brain-inspired computing, hardware-software co-design, programming techniques\\n\\n\\n\\n\\nI Introduction\\n\\n\\nComputing technology is steering toward an impasse, with Dennard scaling ending and Moore’s law slowing down [1]. The impasse gives rise to innovation opportunities for specialized hardware in computer architecture [2] as well as in software [3].\\nThis ‘Golden Age’ of innovation has led many researchers to investigate neuromorphic computers. Taking inspiration from how the brain computes has a rich history [4] and the recent success of deep learning has demonstrated the power of neural information processing [5].\\nThe development of event-based sensors, large-scale neuromorphic processors, and brain-computer interfaces indicate that neuromorphic computing is on the rise and expected to play an important role in the future of computing [6].\\n\\n\\nNeuromorphic computers take inspiration from the brain, both in the way information is processed and in the fact that the physical dynamics of the underlying substrate are exploited for computation [7].\\nA neuromorphic computer is composed of neurons and synapses which model biological neural networks.\\nThe hardware can either directly implement these biological neural network models physically [8] or use them as inspiration and guidelines to design a more general, flexible, and configurable architecture [9]. Key features of these biological networks include sparse connectivity, event-based communication, low-precision activations, and a focus on temporal processing.\\n\\n\\nThe departure from fundamental assumptions in classical computing brings an urgent need for new theories to describe the computations in novel neuromorphic hardware, along with new theories and methods of programming that can make these devices useful.\\nThe former has been outlined in recent work [10, 7] whereas the latter is constrained to an as-yet limited set of “spiking neuromorphic algorithms” [11, 12]. Schuman et al. [11] argue that progress on neuromorphic programming requires a paradigm shift in how to think about programming.\\nCurrent programming methods are adapted to clocked digital hardware, but with the forthcoming diversity of computer hardware and architectures [2] it is time to widen the set of hardware systems supported by our programming models.\\n\\n\\nFigure 1: \\nProgramming and computational models plotted against their operational domain (continuous vs. analog) and the malleability of the computation during execution (immutable vs. plastic).\\nThe class of neuromorphic systems is shown as a green ellipse, spanning both the continuous and discrete domains.\\nThere is a lack of models for continuous and plastic systems.\\n\\n\\n\\nHowever, we need not start from scratch when designing methods and paradigms for neuromorphic programming. A lot of work has already been done that we can draw inspiration from [9], and new hardware is known to rejuvenate unconventional research ideas, as demonstrated by GPUs reviving research on neural networks trained with gradient descent [13, 5]. With the emergence of cutting-edge neuromorphic hardware, revisiting unconventional programming paradigms can uncover promising new directions [14].\\n\\n\\nIn this work, we discuss the practical implications of programming neuromorphic systems and compare them with conventional methods in computer science.\\nWe extend traditional concepts of programming to embrace the uniquely physical and adaptive properties of neuromorphic systems so as to leverage these systems more effectively.\\nOur contributions are twofold: first, we provide a detailed conceptual framework in Section III that redefines programming in the context of neuromorphic hardware, challenging traditional paradigms and encouraging the adoption of novel approaches. Second, we identify and discuss advanced programming models in Section IV that are currently underutilized in the field, proposing new directions for their evolution and integration into mainstream computing practices.\\n\\n\\n\\n\\nII Motivation\\n\\n\\nNeuromorphic hardware leverages the computational principles of the brain, which are vastly different from those exploited by conventional digital computers [4].\\nThese fundamental differences pose challenges for traditional programming abstractions, and it is clear that we cannot apply conventional theoretical computer science to understand the computational processes in neuromorphic systems [15].\\nWe highlight five fundamental differences underpinning this incompatibility.\\n\\n\\nDomain\\n\\nNeurons are physical systems and operate in continuous time (see Figure 1).\\nThe merits of analog computation in terms of energy efficiency and inherent parallelism are well-known [16, 17]. Classical symbolic computation, in contrast, is decoupled from real physical time and time is only simulated through a discrete global clock signal.\\n\\n\\n\\nPlasticity\\n\\nWhen programming digital computers, one may neglect the physical properties of the underlying hardware, as the underlying hardware does not change during execution.\\nIn neuromorphic computers, such hardware-agnostic programming is not generally possible, as these devices are by definition physical.\\nAny programming model of neuromorphic systems must, therefore, include the malleability of the system, since the physical system and the computational model are one and the same.\\n\\n\\n\\nStochasticity\\n\\nUnlike deterministically switching transistors, neural systems are stochastic, which has lead to models of computation with probabilistic logic [18] and stochastic computing [19], where information is represented in probability distributions.\\n\\n\\n\\nDecentralization\\n\\nThe distributedness of information representation and processing in neural networks stands in contrast to the localized information in binary transistor states and the sequential execution of elementary instructions in digital hardware. Such distributedness is leveraged in deep learning [5], in dynamic neural fields [20] where neurons are considered independent independently evolving dynamical systems, and in hyperdimensional computing [21] where high-dimensional random vectors are used for information representation and computation.\\n\\n\\n\\nUnobservability\\n\\nNeuromorphic hardware, especially analog and mixed-signal systems, often show limited observability, in that the system state can only be read out in parts. This is a key difficulty for plastic computations that change over time. It also relates to mismatches between platforms—a known challenge for analog devices that can also be observed between digital systems [22].\\n\\n\\nFigure 1 fits programming and computational concepts onto two axes: plasticity and domain.\\nIt is immediately clear that the upper right quadrant is largely left unexplored.\\nTo explain the vacuum, it is necessary to expand our concept of classical computation to include neuromorphic, physical systems.\\n\\n\\n\\n\\n\\nIII Theoretical framework\\n\\n\\nThe present section introduces a theoretical framework that abstracts “computation” to capture both classical and neuromorphic systems.\\nWe begin by conceptualizing how neuromorphic computing fits within the broader landscape of computational models, highlighting areas that remain underexplored and elucidating why these gaps exist.\\n\\n\\n\\nIII-A Computing with physical systems\\n\\n\\nHorsman et al. [23] provide a general framework for computation with arbitrary physical systems.\\nTherein, a computer is a physical machine ΨΨ\\\\Psiroman_Ψ which can be stimulated by an input signal uΨsuperscript𝑢Ψu^{\\\\Psi}italic_u start_POSTSUPERSCRIPT roman_Ψ end_POSTSUPERSCRIPT and from which an output signal yΨsuperscript𝑦Ψy^{\\\\Psi}italic_y start_POSTSUPERSCRIPT roman_Ψ end_POSTSUPERSCRIPT can be read out.\\nThe computation λ𝜆\\\\lambdaitalic_λ is specified by an abstract function from input uλsuperscript𝑢𝜆u^{\\\\lambda}italic_u start_POSTSUPERSCRIPT italic_λ end_POSTSUPERSCRIPT to output yλsuperscript𝑦𝜆y^{\\\\lambda}italic_y start_POSTSUPERSCRIPT italic_λ end_POSTSUPERSCRIPT.\\nThe machine ΨΨ\\\\Psiroman_Ψ then implements the computation λ𝜆\\\\lambdaitalic_λ if an encoding procedure E𝐸Eitalic_E and decoding procedure D𝐷Ditalic_D is known such that the machine ΨΨ\\\\Psiroman_Ψ will produce yΨsuperscript𝑦Ψy^{\\\\Psi}italic_y start_POSTSUPERSCRIPT roman_Ψ end_POSTSUPERSCRIPT with D\\u2062(yΨ)≈yλ𝐷superscript𝑦Ψsuperscript𝑦𝜆D(y^{\\\\Psi})\\\\approx y^{\\\\lambda}italic_D ( italic_y start_POSTSUPERSCRIPT roman_Ψ end_POSTSUPERSCRIPT ) ≈ italic_y start_POSTSUPERSCRIPT italic_λ end_POSTSUPERSCRIPT when stimulated with the input signal E\\u2062(uλ)=uΨ𝐸superscript𝑢𝜆superscript𝑢ΨE(u^{\\\\lambda})=u^{\\\\Psi}italic_E ( italic_u start_POSTSUPERSCRIPT italic_λ end_POSTSUPERSCRIPT ) = italic_u start_POSTSUPERSCRIPT roman_Ψ end_POSTSUPERSCRIPT.\\nThis leads to the general form of the abstract computer model shown in Figure 2: the physical machine ΨΨ\\\\Psiroman_Ψ receives input uΨsuperscript𝑢Ψu^{\\\\Psi}italic_u start_POSTSUPERSCRIPT roman_Ψ end_POSTSUPERSCRIPT and produces output yΨsuperscript𝑦Ψy^{\\\\Psi}italic_y start_POSTSUPERSCRIPT roman_Ψ end_POSTSUPERSCRIPT, thereby implementing the abstract computation λ𝜆\\\\lambdaitalic_λ from input uλsuperscript𝑢𝜆u^{\\\\lambda}italic_u start_POSTSUPERSCRIPT italic_λ end_POSTSUPERSCRIPT to output yλsuperscript𝑦𝜆y^{\\\\lambda}italic_y start_POSTSUPERSCRIPT italic_λ end_POSTSUPERSCRIPT.\\n\\n\\nAny machine that is changing during program execution will, according to Horsman et al. require either that the machine is insensitive to the change such that the effective input-output function stays unaffected, or that the computational model includes the change, so the computation stays correct.\\nThus, to fit the framework of Horsman et al., neuromorphic computing first requires a model that captures their (plastic) operation. This can be expressed in the formalism of (non-autonomous) dynamical systems, logic-based systems or other formalisms [10].\\n\\n\\nFor digital computers, the abstract model of computation was developed first and only later physically realized.\\nIn contrast, neuromorphic hardware does not rely on any universally accepted abstract model [24]. Abstract models of computation are co-developed with physical implementations [25].\\n\\n\\n\\n\\nIII-B Computer programs\\n\\n\\nWhile a computation 𝒞𝒞\\\\mathcal{C}caligraphic_C specifies what is being computed, a program 𝒫𝒫\\\\mathcal{P}caligraphic_P specifies how the computation is implemented. Many different programs 𝒫1,…,𝒫nsubscript𝒫1…subscript𝒫𝑛\\\\mathcal{P}_{1},\\\\ldots,\\\\mathcal{P}_{n}caligraphic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , caligraphic_P start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT may implement the same computation 𝒞𝒞\\\\mathcal{C}caligraphic_C.\\nNote that the concept of a ‘program’ herein includes algorithms as Turing machines as well as learning algorithms and interactive programs—although the latter two cannot be implemented by Turing machines [26, 27].\\n\\n\\nA computation 𝒞𝒞\\\\mathcal{C}caligraphic_C is described by a formal specification that formalizes the intention of the computation (Figure 3). The specification of a computation is expressed in some mathematical formalism. In digital computing, this can be done using formalisms from logic. In analog computing, there are various formalisms that describe the computation, for example qualitative geometrical constructs like attractors and bifurcations [10].\\n\\n\\nA program 𝒫𝒫\\\\mathcal{P}caligraphic_P is described in another formalism. In digital computing, programs are expressed in some programming language. In analog computing, one typically uses differential equations to describe the program. When programs interact with another, one also speaks of each individual program as a process and the ensemble of all processes as the program, whose behavior emerges from the interaction of the processes.\\n\\n\\nOperationally, a program is defined by the data flow and control flow.\\nThe data flow specifies how signals that carry computationally relevant information are propagated through the machine.\\nThe control flow specifies what operations or transformations are done on these signals.\\nFor example, in a field-programmable gate array (FPGA) the data flow is configured through its routing elements while the control flow is defined by the function implemented in each logic block.\\nIn a neuromorphic chip, the data flow is defined by the connectivity of the neural network while the control flow is defined by the synapse and neuron models, learning rules, synaptic weights, time constants, thresholds, and more.\\n\\n\\n\\n\\n\\nIII-C Programming process\\n\\n\\nProgramming, in the context of the theoretical framework above, is the process of designing how a certain computation should be implemented, illustrated in\\nFigure 3. Programming begins with some informal intention of what computation to implement. This intention can be formalized into a specification (right path in Figure 3), or the programmer may directly come up with an informal idea for a program that implements the intended computation, and then code this idea in some formal language (left path in Figure 3). This program is then communicated to the physical computer through a pre-defined programming interface. Finally, the system executing this program can be controlled or instructed to remain within the program’s specification.\\n\\n\\nProgramming need not be done by a human programmer. As shown by the green arrows in the diagram, programming can also be automated: a computer program can take in a formal specification of a program to then create a program that satisfies these specifications.\\nIn program synthesis, the specification is given in some algebra or formal model and a search algorithm then finds a program that meets this specification.\\nIn supervised machine learning, the specification is given by a dataset of input-output examples and an error function, which is then minimized for some machine learning model until it meets a given error threshold.\\n\\n\\nFigure 2: The relationship between an abstract computation λ:uλ↦yλ:𝜆maps-tosuperscript𝑢𝜆superscript𝑦𝜆\\\\lambda:u^{\\\\lambda}\\\\mapsto y^{\\\\lambda}italic_λ : italic_u start_POSTSUPERSCRIPT italic_λ end_POSTSUPERSCRIPT ↦ italic_y start_POSTSUPERSCRIPT italic_λ end_POSTSUPERSCRIPT (middle), the same computation expressed in a different computational model λ∗superscript𝜆\\\\lambda^{*}italic_λ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT such as a higher-level programming language (top), and the physical computer ΨΨ\\\\Psiroman_Ψ that realizes this computation (bottom), from [23].\\n\\n\\n\\n\\nIII-D Languages and Paradigms\\n\\n\\nConventionally, programming amounts to writing source code, coding, in some formal language. Here, ‘programming language’ is used in an unconventionally wide sense to include any formal language that can be communicated to a physical system.\\nThis includes programming languages like Python but also extends to other formalisms like differential equations describing dynamical systems, or block diagrams describing signal processing systems.\\nIn any case, the ‘programming language’ must be compatible with the elementary instructions that the computer’s programming interface provides. Given this compatibility, the programmer is free to explore the space of all possible programs.\\nWork on elementary instruction sets for non-digital computers goes back at least to the 1940s and continues to the present day [28, 29] but there is still no universally accepted model [10].\\nConsequently, it is not clear what a neuromorphic programming language may look like [30]; will it require new syntax such as visual representations, or will a program be represented by a string of symbols in some formal language?\\n\\n\\nSince the goal is to improve the way we think about programming neuromorphic computers in general, Floyd [31] argued that it is more effective to turn to programming paradigms rather than programming languages.\\nA programming paradigm is an approach to programming “based on a mathematical theory or a coherent set of principles” [32] and a programming language implements one or more programming paradigms.\\nCentering the discussion on programming paradigms shifts the focus away from syntactical issues to the way programs are conceived and designed.\\n\\n\\nFigure 3: The process of computer programming, inspired by [33]. The abstract intention lives in the informal space ΩΩ\\\\Omegaroman_Ω of all possible computational tasks. This intention is formalized into a program p∈λ𝑝𝜆p\\\\in\\\\lambdaitalic_p ∈ italic_λ that is expressed in some formal language, which is then implemented through some physical system s∈Ψ𝑠Ψs\\\\in\\\\Psiitalic_s ∈ roman_Ψ.\\n\\n\\nBeyond languages and paradigms, there is a set of well-developed tools for computer programming without which most modern software systems would not exist. We mention the integrated development environment (IDE), the keyboard-and-mouse interface, version control systems, agile development only in passing, as a more detailed treatment of these is outside the scope of this paper.\\n\\n\\n\\n\\n\\nIV Programming Paradigms\\n\\n\\nIn the present section, existing programming paradigms are explored and then related to Neuromorphic Programming in Section V.\\nConventional computer programming paradigms can be differentiated by the program’s representation.\\nOne commonly distinguishes imperative programming, where a program consists of a sequence of instructions, from declarative programming, where a program consists of a sequence of logical assignments.\\nWe use the term decentralized programming for parallel, concurrent, and distributed programming, spanning both imperative and declarative styles.\\nWe distinguish these paradigms from automated programming, in which the human programmer passes a (potentially incomplete) specification into an optimization or learning algorithm that creates the final program.\\nFinally, we also give a brief overview of non-digital programming methods that are applicable to analog and physical computers.\\n\\n\\n\\nIV-A Imperative programming\\n\\n\\nThe most common way of writing sequential, instruction-based programs uses the imperative paradigm, as implemented in C.\\nImperative programming was augmented with objects, which can contain instructions as well as data, to yield the object-oriented paradigm, as implemented in C++ or Java. Imperative programming is the dominant paradigm for large-scale software systems in conventional computing.\\n\\n\\n\\n\\nIV-B Declarative\\n\\n\\nInstead of describing the control flow of a program in imperative programming, declarative programs describe the logic of the program. A declarative program describes what the program does rather than how it does it. Declarative programming is done in database query languages like SQL, functional programming languages like Haskell, or logic programming languages like Prolog.\\nIn dataflow programming, a program is modeled as a graph of data flowing between operations.\\nA functional cousin of dataflows is the functional reactive programming (FRP) that couples hybrid systems of behaviors and events with continuous-time flow [34].\\nDataflow programming is well-suited for neuromorphic computers, where data flows between neurons.\\n\\n\\nWhile classical programs are deterministic, the execution of a probabilistic program depends on random numbers, for example by calling a (pseudo) random number generator. Such a program can be viewed as sampling from a probability distribution. In probabilistic programming, the program itself is considered a distribution, and the programmer can analyze this distribution and condition the distribution on observations [35]. Indeed, the goal of probabilistic programming is not simply the execution of a program, but also the analysis thereof.\\n\\n\\n\\n\\nIV-C Decentralized programming\\n\\n\\nA single processor core may use time-sharing to allow multiple programs to run concurrently. In so-called concurrent programming, the lifetime of multiple computing processes overlap and may interact with another [36]. Concurrency introduces issues of synchronization, such as deadlocks and race conditions.\\nWith the advent of multicore microprocessors came the need to use resources on different cores simultaneously. This led to the development of parallel programming techniques, in which multiple processes are carried out simultaneously on different cores. Distributed programming deals with programs that are executed on multiple networked computers, which interact to achieve a common goal.\\nThe first approach to formalizing this appeared by Milner et al. as π𝜋\\\\piitalic_π-calculus [37].\\nMethods from distributed programming are used for multi-chip systems, and frequently employed in training large-scale neural networks.\\n\\n\\n\\n\\nIV-D Automated programming\\n\\n\\nIn meta-programming, it is possible for a program to write or modify programs, by simply treating the program as data.\\nIn reflective programming, a program modifies its own behavior whereas in automatic programming, a program generates another program.\\nIf a formal specification of the desired program is given, program synthesis can be used to generate a program that provably satisfies this specification [38].\\nIf exact adherence to a formal specification is not required, but only the satisfaction of given constraints, constraint programming may be used [39].\\nIf an incomplete specification is available, such as input-output examples, then inductive programming can be used to generate a suitable candidate program [40].\\n\\n\\nIn classical programming, a human programmer defines the program that specifies how input data is processed. Machine learning constructs programs that learn from the input data, in ways that may not have been anticipated by any human. Machine learning has deep roots in probability theory and overlaps significantly with probabilistic programming [41].\\nIn supervised machine learning, a mapping from inputs to outputs is learned from a set of examples.\\nIn reinforcement learning, a policy of how to act in some environment is learned from rewards and punishments.\\nBoth the learned mapping in supervised learning and the learned policy in reinforcement learning can be used as programs. This makes machine learning a paradigm for automated programming. Deep learning uses multi-layered artificial neural networks (ANNs) for machine learning. The connectivity in ANNs is usually fixed, and the weights are learned, typically in a supervised fashion using gradient descent, to minimize the error on given input-output examples.\\nThe techniques of deep learning have also been adapted for spiking neural networks [42].\\nIn differentiable programming, programs are written in a way that they are fully differentiable with respect to some loss function, thereby allowing the use of gradient-based optimization methods to find better-performing programs. Deep learning is a special case of this, where programs are artificial neural networks that are differentiated using backpropagation.\\n\\n\\nIn machine learning, a core goal is good generalization to unseen examples.\\nIf generalization is not needed, one may use optimization itself as a programming paradigm in which the result of the optimization is the desired program or the optimization process itself.\\nFor example, evolutionary programming uses population-based evolutionary optimization algorithms to find programs by encoding the program’s specification in a fitness function that is optimized. Evolutionary algorithms have been used to generate rules for a cellular automaton to solve computational problems that are difficult to solve by manually designing a learning rule [43].\\n\\n\\n\\n\\nIV-E Non-digital programming\\n\\n\\nBuilding on evolutionary optimization, evolution in materio [44] was proposed to harness material properties for computation. Natural evolution excels in exploiting the physical properties of materials, and artificial evolution aims to emulate this ability.\\nPhysical reservoir computing can be used to harness the dynamics of physical systems for computation by modeling the physical system as a high-dimensional reservoir for which a linear readout is trained [45].\\nIt is also possible to create a surrogate model of the physical device, then optimize the surrogate model in simulation with deep learning methods and transfer the optimized model back to the device [46].\\nAlthough analog computers have been around for at least as long as their digital counterparts, analog programming methods are not at the same level of maturity as those for digital computers. Ulmann [47] argues that the development of reconfigurable analog computers will advance the state of analog computer programming, and efforts to develop such hardware already exist [48].\\nCurrently, analog programming often draws on methods from control engineering, signal processing and cybernetics.\\nFor analog neuromorphic computers, signal processing provides a rich framework for computing with temporal signals [49].\\nMoreover, control theory has developed a rich repertoire of methods to drive a dynamical system into a mode of operation that is robust, stable, and implements some desired dynamics [50]. These methods can be used to keep analog computers within a desired regime of operation to implement a desired computation.\\nHowever, the expressiveness of behaviors in control theory lags far behind that of digital programming languages.\\nThe field of autonomic computing aims to design systems that adapt to stay within a high-level description of desired behavior [51]. The field takes inspiration from the autonomic nervous system, which can stay within a stable ‘dynamic equilibrium’ without global top-down control.\\n\\n\\n\\n\\n\\nV Neuromorphic Programming\\n\\n\\nThe preceding section already alluded to some methods that have been adopted for neuromorphic hardware, such as reservoir computing, deep learning, and evolutionary optimization. In the present section, we give a more in-depth review of existing programming approaches for neuromorphic hardware.\\n\\n\\nNeuromorphic co-design\\n\\nAs neuromorphic computers exploit physical phenomena of their underlying hardware, manually designed neuromorphic programs will necessarily be close to physics. Therefore, although not strictly a ‘programming’ paradigm, it is instructive to consider neuromorphic co-design as a paradigm for designing neuromorphic systems.\\nThe field is rooted in the original vision of neuromorphic computing [52] and designs application-specific and reconfigurable mixed-signal neuromorphic chips in sub-threshold CMOS technology.\\nThis approach uses tools from signal processing and computational neuroscience to implement a desired behavior in networks of silicon neurons [8].\\n\\n\\n\\nMachine learning methods\\n\\nGiven the success of deep learning in applying machine learning techniques to neural networks, this is a natural starting point for neuromorphic computers.\\nHowever, it is unrealistic to expect deep learning methods to work for SNNs as well as they do for ANNs since these methods were optimized for ANNs [53].\\nANN-to-SNN conversion is possible, but typically not optimal because the resulting SNNs do not leverage the computational power of spiking neurons. Instead, they limit the richer dynamics of SNNs to the less expressive domain of ANNs [11].\\nOffline training methods like backpropagation, the workhorse of deep learning, can be implemented directly in SNNs using surrogate gradients [42].\\nGiven a neural network, it is necessary to communicate this network to the hardware.\\nNeuromorphic compilation [54] was proposed as a general framework to (approximately) compile neural networks into different hardware systems, automatically adapting to physical constraints.\\nReservoir computing can simplify the training of SNNs on hardware, but it still requires the reservoir states to be read out and stored on a digital computer, which may not be possible given limited observability of some devices or limited data storage.\\n\\n\\n\\nOnline learning\\n\\nThe machine learning methods presented above all operate offline and often off-device. Frequent re-training creates a large overhead, limiting the performance and applicability of neuromorphic computers.\\nAs a result, on-device learning methods are an active topic of research [55].\\nPlasticity is a popular paradigm for on-device learning, where local learning rules are used to modify the connectivity (structural plasticity) and connection strengths (synaptic plasticity) of a SNN.\\nParallels to emergent programming may be drawn here, as the resulting behavior of the SNN emerges from the interaction of local rules. It is not clear what local rules will yield a particular network-level behavior, but evolutionary search [56] and meta-learning [57] have been used to (re-)discover desirable plasticity rules.\\n\\n\\n\\nEvolutionary methods\\n\\nA key advantage of evolutionary approaches is that they can jointly optimize the network’s architecture and weights, thus simultaneously designing and training the network without requiring the network to be differentiable.\\nHowever, evolutionary approaches can be slower to converge than other training methods and the resulting architectures are not easily interpretable or reusable for different tasks [11].\\n\\n\\n\\nSpiking neuromorphic algorithms\\n\\nWith the increased availability of neuromorphic hardware, a number of handcrafted spiking neuromorphic algorithms (SNA) have been proposed. SNAs implement computations using temporal information processing with spikes, often to implement well-defined computations such as functions on sets of numbers [58], functions on graphs [59], solving constraint satisfaction problems or solving a steady-state partial differential equation using random walks [60].\\n\\n\\n\\nNeurocomputational primitives\\n\\nVarious neurocomputational primitives have been proposed in the neuromorphic community. Such primitives can be useful for simple tasks and can be combined into complex neuromorphic systems [61].\\nThe winner-take-all (WTA) network is a common circuit motive in the neocortex that has been used extensively for neuromorphic systems [62]. The more general dynamic neural fields (DNFs) are a modern framework for neural attractor networks [20].\\nThe neural state machine (NSM) [63] also builds on WTA networks to implement finite state machines in SNNs.\\nThe temporal difference encoder (TDE) [64] is a circuit primitive that has been used for motion estimation and obstacle avoidance.\\nNeural oscillators generate rhythmic activity that can be used for feature binding and motor coordination, for example as a central pattern generator [65].\\nOther primitives are scattered around the literature, and shared libraries of neurocomputational primitives are only starting to be assembled [61].\\n\\n\\n\\nHigher abstractions\\n\\nThe neural engineering framework [66] raises the level of abstraction beyond networks of neurons—it allows dynamical systems to be automatically distilled into networks of spiking neurons using the Nengo programming environment [67].\\nThe framework of vector symbolic architectures (VSA) is suitable for neuromorphic systems, enabling knowledge representation and reasoning in high-dimensional spaces.\\nVSAs interfaced with deep networks and generalizations of the optimization and search algorithms described in this survey could provide a path to enabling fast, efficient, and scalable next-generation AI capabilities on neuromorphic hardware.\\nLava is an open-source neuromorphic programming framework that includes libraries of neuromorphic algorithms for optimization, attractor networks, deep learning methods for SNNs, VSAs, and plans to include more paradigms.\\nFugu [68] is a hardware-independent mechanism for composing SNAs. In Fugu, a program is specified as a computational graph, reminiscent of dataflow programming, where nodes represent SNAs and connections represent dataflow between the SNAs.\\n\\n\\n\\n\\n\\nVI Future approaches to programming brain-inspired hardware\\n\\n\\nFor neuromorphic systems to scale to large heterogeneous computing systems, commonly agreed-upon computational models are required, similar to how programming abstractions catalyzed the development of the digital computer in the 1950s and 60s.\\nWe have argued that it is difficult to program computers that harness their underlying physical dynamics for computation without a guiding theory that unites physics with computation.\\nBut, despite the rich history of programming methods and languages, our analyses in Sections II and III show that the direct translation of classical methods to neuromorphic systems is not a reasonable pursuit: neuromorphic computing (dually analog/digital, plastic, stochastic, decentralized, and unobservable) is incompatible with the assumptions of conventional programming efforts (digital, immutable, noiseless, centralized, observable, see Figure 1).\\nNone of the methods and approaches we reviewed meet the criteria for a universally agreed-upon neuromorphic abstraction [6, 11].\\n\\n\\nHowever, decades of research in neuromorphic computing and engineering, provide important insights towards initial features of neuromorphic programming methods.\\nRevisiting the requirements from Section II, modern approaches must accommodate the simultaneously analog and digital nature of neural computation to supplement digital models with differential equations in some shape or form.\\nThis does not exclude digital instructions, but implies approximately continuous-time primitives, such as linearized time-stepped or variable-time models.\\nSecond, the computational models must be able to capture the inherent change in the underlying substrate.\\nThis further challenges digital instructions because arbitrary bit-flips are detrimental to digital computation.\\nOnline learning methods may help by continuously adapting to changes in the environment as well as changes in the underlying substrate.\\nThird, novel approaches should model noise on the signal-level and remain robust to small and unpredictable perturbations.\\nThis is challenging, but possible, to achieve in classical programs, and much more applied in machine learning and automated programming schemes, and even harnessed in probabilistic programming and stochastic computing.\\nFourth, any model should allow for event-based information processing. Synchronicity in nature is extremely costly, but used almost everywhere in conventional programming paradigms except some reactive and concurrent languages.\\nFinally, the computation should depend solely on locally available information rather than on global system states, as done by local plasticity rules used in neuromorphic hardware.\\nSome machine learning and optimization methods, like reservoir computing or evolutionary optimization, also do not require a complete description of the computer’s state.\\n\\n\\nTo unite the physical dynamics and computational principles, fully neuromorphic abstractions are preferable, but probably not tractable before the “hardware lottery” has been won [13] and significant gains have been achieved with neuromorphic hardware compared against other computational substrates.\\nA more realistic scenario is that we can play on the strengths of multiple current approaches, and integrate them into one cohesive abstraction.\\nThe neuromorphic system hierarchy [54] provides a common abstraction that can represent ANNs and SNNs, but is not designed for neurocomputational primitives, spiking neuromorphic architectures, or other neuromorphic abstractions.\\nThe Neuromorphic Intermediate Representation [22] provides an abstraction for graph-structured continuous-time computations, which can support a wider variety of neuromorphic programming paradigms.\\nA key innovation in NIR is the integration of heterogeneous representations, such the Neural Engineering framework, Lava, Fugu, PyNN, and NeuroML.\\nThe outcome is that multiple representations and paradigms can cooperate through a shared representation, flexible enough to cover multiple approaches, but unambiguous enough to only provide limits for discrepancies in the execution.\\nHowever, a complete/full neuromorphic abstraction must also offer support for computational graphs that change over time, which NIR presently does not offer.\\n\\n\\nA longstanding goal in computer science has been to program physical devices that mimic the efficiency and functionality of the brain [4, 7].\\nWhile significant headway has been made towards instrumenting digital as well as non-digital systems, more work is needed to find robust programming methods for neuromorphics.\\nWe hope that neuromorphic programmers can leverage the work outlined in this paper to build large-scale neuromorphic programs to tackle real-world tasks, and to further develop guiding principles and paradigms for neuromorphic programming.\\n\\n\\n\\nAcknowledgment\\n\\nS.A. thanks Herbert Jaeger, Guillaume Pourcel, and Mirko Goldmann for helpful comments and discussions.\\nS.A. gratefully acknowledges funding from the European Union’s Horizon 2020 Research and Innovation Programme under the Marie Skłodowska-Curie grant agreement No. 860360 (POST DIGITAL).\\nJ.P. would like to thank funding by the EC Horizon 2020 Framework Programme under Grant Agreements 785907 and 945539 (HBP) and NeuroPAC under the NSF grant\\n“AccelNet: Accelerating Research on Neuromorphic Perception, Action, and Cognition.”\\n\\n\\n\\nReferences\\n\\n\\n[1]\\n\\nM.\\xa0M. Waldrop, “The chips are down for moore’s law,” Nature, vol. 530, no. 7589, pp. 144–147, 2016.\\n\\n\\n\\n\\n[2]\\n\\nJ.\\xa0L. Hennessy and D.\\xa0A. Patterson, “A new golden age for computer architecture,” Communications of the ACM, vol.\\xa062, no.\\xa02, pp. 48–60, 2019.\\n\\n\\n\\n\\n[3]\\n\\nC.\\xa0Edwards, “Moore's law,” Communications of the ACM, vol.\\xa064, no.\\xa02, pp. 12–14, 2021.\\n\\n\\n\\n\\n[4]\\n\\nJ.\\xa0von Neumann, The computer and the brain.\\xa0\\xa0\\xa0New Haven, CT: Yale University Press, 1958.\\n\\n\\n\\n\\n[5]\\n\\nY.\\xa0LeCun, Y.\\xa0Bengio, and G.\\xa0Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–444, 2015.\\n\\n\\n\\n\\n[6]\\n\\nC.\\xa0Mead, “Neuromorphic engineering: In memory of Misha Mahowald,” Neural Computation, vol.\\xa035, no.\\xa03, p. 343–383, Feb. 2023.\\n\\n\\n\\n\\n[7]\\n\\nH.\\xa0Jaeger, B.\\xa0Noheda, and W.\\xa0G. v.\\xa0d. Wiel, “Toward a formal theory for computing machines made out of whatever physics offers,” Nature Communications, vol.\\xa014, no.\\xa01, 2023.\\n\\n\\n\\n\\n[8]\\n\\nG.\\xa0Indiveri, B.\\xa0Linares-Barranco, T.\\xa0J. Hamilton, A.\\xa0van Schaik, R.\\xa0Etienne-Cummings, T.\\xa0Delbruck, S.-C. Liu, P.\\xa0Dudek, P.\\xa0Häfliger, S.\\xa0Renaud, J.\\xa0Schemmel, G.\\xa0Cauwenberghs, J.\\xa0Arthur, K.\\xa0Hynna, F.\\xa0Folowosele, S.\\xa0Saighi, T.\\xa0Serrano-Gotarredona, J.\\xa0Wijekoon, Y.\\xa0Wang, and K.\\xa0Boahen, “Neuromorphic silicon neuron circuits,” Frontiers in Neuroscience, vol.\\xa05, 2011.\\n\\n\\n\\n\\n[9]\\n\\nC.\\xa0Frenkel, D.\\xa0Bol, and G.\\xa0Indiveri, “Bottom-up and top-down neural processing systems design: Neuromorphic intelligence as the convergence of natural and artificial intelligence,” 2021.\\n\\n\\n\\n\\n[10]\\n\\nH.\\xa0Jaeger, “Towards a generalized theory comprising digital, neuromorphic and unconventional computing,” Neuromorphic Computing and Engineering, 2021.\\n\\n\\n\\n\\n[11]\\n\\nC.\\xa0D. Schuman, S.\\xa0R. Kulkarni, M.\\xa0Parsa, J.\\xa0P. Mitchell, P.\\xa0Date, and B.\\xa0Kay, “Opportunities for neuromorphic computing algorithms and applications,” Nature Computational Science, vol.\\xa02, no.\\xa01, pp. 10–19, 2022.\\n\\n\\n\\n\\n[12]\\n\\nJ.\\xa0B. Aimone, “Neural algorithms and computing beyond moore's law,” Communications of the ACM, vol.\\xa062, no.\\xa04, pp. 110–110, 2019.\\n\\n\\n\\n\\n[13]\\n\\nS.\\xa0Hooker, “The hardware lottery,” Commun. ACM, vol.\\xa064, no.\\xa012, p. 58–65, nov 2021.\\n\\n\\n\\n\\n[14]\\n\\nJ.-P. Banâtre, P.\\xa0Fradet, J.-L. Giavitto, and O.\\xa0Michel, Eds., Unconventional Programming Paradigms.\\xa0\\xa0\\xa0Springer Berlin Heidelberg, 2005.\\n\\n\\n\\n\\n[15]\\n\\nJ.\\xa0B. Aimone and O.\\xa0Parekh, “The brain’s unique take on algorithms,” Nature Communications, vol.\\xa014, no.\\xa011, p. 4910, Aug. 2023.\\n\\n\\n\\n\\n[16]\\n\\nK.\\xa0Boahen, “A neuromorph's prospectus,” Computing in Science & Engineering, vol.\\xa019, no.\\xa02, pp. 14–28, 2017.\\n\\n\\n\\n\\n[17]\\n\\nR.\\xa0Sarpeshkar, “Analog versus digital: Extrapolating from electronics to neurobiology,” Neural Computation, vol.\\xa010, no.\\xa07, pp. 1601–1638, 1998.\\n\\n\\n\\n\\n[18]\\n\\nJ.\\xa0von Neumann, “Probabilistic logics and the synthesis of reliable organisms from unreliable components,” Automata Studies, pp. 43–98, 1956.\\n\\n\\n\\n\\n[19]\\n\\nA.\\xa0Alaghi and J.\\xa0P. Hayes, “Survey of stochastic computing,” ACM Transactions on Embedded Computing Systems, vol.\\xa012, no.\\xa02s, pp. 1–19, 2013.\\n\\n\\n\\n\\n[20]\\n\\nM.\\xa0A. Giese, “Dynamic neural field theory for motion perception.”\\xa0\\xa0\\xa0Boston, MA: Springer US, 1999, dOI: 10.1007/978-1-4615-5581-0.\\n\\n\\n\\n\\n[21]\\n\\nP.\\xa0Kanerva, “Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,” Cognitive Computation, vol.\\xa01, no.\\xa02, pp. 139–159, 2009.\\n\\n\\n\\n\\n[22]\\n\\nJ.\\xa0E. Pedersen, S.\\xa0Abreu, M.\\xa0Jobst, G.\\xa0Lenz, V.\\xa0Fra, F.\\xa0C. Bauer, D.\\xa0R. Muir, P.\\xa0Zhou, B.\\xa0Vogginger, K.\\xa0Heckel, G.\\xa0Urgese, S.\\xa0Shankar, T.\\xa0C. Stewart, J.\\xa0K. Eshraghian, and S.\\xa0Sheik, “Neuromorphic Intermediate Representation: A Unified Instruction Set for Interoperable Brain-Inspired Computing,” Nov. 2023, arXiv:2311.14641 [cs].\\n\\n\\n\\n\\n[23]\\n\\nC.\\xa0Horsman, S.\\xa0Stepney, R.\\xa0C. Wagner, and V.\\xa0Kendon, “When does a physical system compute?” Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 470, no. 2169, p. 20140182, 2014.\\n\\n\\n\\n\\n[24]\\n\\nC.\\xa0D. Schuman, T.\\xa0E. Potok, R.\\xa0M. Patton, J.\\xa0D. Birdwell, M.\\xa0E. Dean, G.\\xa0S. Rose, and J.\\xa0S. Plank, “A survey of neuromorphic computing and neural networks in hardware,” arXiv preprint, 2017.\\n\\n\\n\\n\\n[25]\\n\\nS.\\xa0Abreu, I.\\xa0Boikov, M.\\xa0Goldmann, T.\\xa0Jonuzi, A.\\xa0Lupo, S.\\xa0Masaad, L.\\xa0Nguyen, E.\\xa0Picco, G.\\xa0Pourcel, A.\\xa0Skalli, L.\\xa0Talandier, B.\\xa0Vettelschoss, E.\\xa0A. Vlieg, A.\\xa0Argyris, P.\\xa0Bienstman, D.\\xa0Brunner, J.\\xa0Dambre, L.\\xa0Daudet, J.\\xa0D. Domenech, I.\\xa0Fischer, F.\\xa0Horst, S.\\xa0Massar, C.\\xa0R. Mirasso, B.\\xa0J. Offrein, A.\\xa0Rossi, M.\\xa0C. Soriano, S.\\xa0Sygletos, and S.\\xa0K. Turitsyn, “A photonics perspective on computing with physical substrates,” Reviews in Physics, vol.\\xa012, p. 100093, Dec. 2024.\\n\\n\\n\\n\\n[26]\\n\\nL.\\xa0Valiant, Probably Approximately Correct: Nature’s Algorithms for Learning and Prospering in a Complex World.\\xa0\\xa0\\xa0Basic Books, Inc., 2013.\\n\\n\\n\\n\\n[27]\\n\\nP.\\xa0Wegner, “Why interaction is more powerful than algorithms,” Communications of the ACM, vol.\\xa040, no.\\xa05, pp. 80–91, 1997.\\n\\n\\n\\n\\n[28]\\n\\nC.\\xa0E. Shannon, “Mathematical theory of the differential analyzer,” Journal of Mathematics and Physics, vol.\\xa020, no. 1-4, pp. 337–354, 1941.\\n\\n\\n\\n\\n[29]\\n\\nJ.\\xa0Hasler, “Defining analog standard cell libraries for mixed-signal computing enabled through educational directions,” in 2020 IEEE International Symposium on Circuits and Systems (ISCAS).\\xa0\\xa0\\xa0IEEE, 2020.\\n\\n\\n\\n\\n[30]\\n\\nO.\\xa0Michel, J.-P. Banâtre, P.\\xa0Fradet, and J.-L. Giavitto, “Challenging Questions for the Rationale of Non-Classical Programming Languages,” International Journal of Unconventional Computing, vol.\\xa02, pp. 337–347, 2006.\\n\\n\\n\\n\\n[31]\\n\\nR.\\xa0W. Floyd, “The paradigms of programming,” Communications of the ACM, vol.\\xa022, no.\\xa08, pp. 455–460, 1979.\\n\\n\\n\\n\\n[32]\\n\\nP.\\xa0V. Roy, “Programming paradigms for dummies: What every programmer should know,” in New computational paradigms for computer music, G.\\xa0Assayag and A.\\xa0Gerzso, Eds.\\xa0\\xa0\\xa0Éditions Delatour France (Le vallier), 2009, pp. 9–47.\\n\\n\\n\\n\\n[33]\\n\\nG.\\xa0Grünert, “Unconventional programming: non-programmable systems,” Ph.D. dissertation, Friedrich-Schiller-Universität Jena, 2017.\\n\\n\\n\\n\\n[34]\\n\\nZ.\\xa0Wan and P.\\xa0Hudak, “Functional reactive programming from first principles,” ser. PLDI ’00.\\xa0\\xa0\\xa0New York, NY, USA: Association for Computing Machinery, May 2000, p. 242–252.\\n\\n\\n\\n\\n[35]\\n\\nA.\\xa0D. Gordon, T.\\xa0A. Henzinger, A.\\xa0V. Nori, and S.\\xa0K. Rajamani, “Probabilistic programming,” in Future of Software Engineering Proceedings.\\xa0\\xa0\\xa0ACM, 2014.\\n\\n\\n\\n\\n[36]\\n\\nR.\\xa0Milner, “Elements of interaction,” Communications of the ACM, vol.\\xa036, no.\\xa01, pp. 78–89, 1993.\\n\\n\\n\\n\\n[37]\\n\\nR.\\xa0Milner, J.\\xa0Parrow, and D.\\xa0Walker, “A calculus of mobile processes, i,” Information and Computation, vol. 100, no.\\xa01, p. 1–40, Sep. 1992.\\n\\n\\n\\n\\n[38]\\n\\nS.\\xa0Gulwani, O.\\xa0Polozov, and R.\\xa0Singh, “Program synthesis,” Foundations and Trends® in Programming Languages, vol.\\xa04, no. 1-2, pp. 1–119, 2017.\\n\\n\\n\\n\\n[39]\\n\\nF.\\xa0Rossi, P.\\xa0van Beek, and T.\\xa0Walsh, “Constraint programming,” in Handbook of Knowledge Representation.\\xa0\\xa0\\xa0Elsevier, 2008, ch.\\xa04, pp. 181–211.\\n\\n\\n\\n\\n[40]\\n\\nS.\\xa0Gulwani, J.\\xa0Hernández-Orallo, E.\\xa0Kitzelmann, S.\\xa0H. Muggleton, U.\\xa0Schmid, and B.\\xa0Zorn, “Inductive programming meets the real world,” Communications of the ACM, vol.\\xa058, no.\\xa011, pp. 90–99, 2015.\\n\\n\\n\\n\\n[41]\\n\\nK.\\xa0P. Murphy, Machine Learning.\\xa0\\xa0\\xa0MIT Press Ltd, 2012.\\n\\n\\n\\n\\n[42]\\n\\nJ.\\xa0K. Eshraghian, M.\\xa0Ward, E.\\xa0O. Neftci, X.\\xa0Wang, G.\\xa0Lenz, G.\\xa0Dwivedi, M.\\xa0Bennamoun, D.\\xa0S. Jeong, and W.\\xa0D. Lu, “Training spiking neural networks using lessons from deep learning,” Proceedings of the IEEE, vol. 111, no.\\xa09, pp. 1016–1054, 2023.\\n\\n\\n\\n\\n[43]\\n\\nM.\\xa0Mitchell, J.\\xa0P. Crutchfield, and P.\\xa0T. Hraber, “Evolving cellular automata to perform computations: mechanisms and impediments,” Physica D: Nonlinear Phenomena, vol.\\xa075, no. 1-3, pp. 361–391, 1994.\\n\\n\\n\\n\\n[44]\\n\\nJ.\\xa0Miller and K.\\xa0Downing, “Evolution in materio: looking beyond the silicon box,” in Proceedings 2002 NASA/DoD Conference on Evolvable Hardware.\\xa0\\xa0\\xa0IEEE, 2002.\\n\\n\\n\\n\\n[45]\\n\\nG.\\xa0Tanaka, T.\\xa0Yamane, J.\\xa0B. Héroux, R.\\xa0Nakane, N.\\xa0Kanazawa, S.\\xa0Takeda, H.\\xa0Numata, D.\\xa0Nakano, and A.\\xa0Hirose, “Recent advances in physical reservoir computing: A review,” Neural Networks, vol. 115, pp. 100–123, 2019.\\n\\n\\n\\n\\n[46]\\n\\nL.\\xa0G. Wright, T.\\xa0Onodera, M.\\xa0M. Stein, T.\\xa0Wang, D.\\xa0T. Schachter, Z.\\xa0Hu, and P.\\xa0L. McMahon, “Deep physical neural networks trained with backpropagation,” Nature, vol. 601, no. 7894, pp. 549–555, 2022, publisher: Springer Science and Business Media LLC.\\n\\n\\n\\n\\n[47]\\n\\nB.\\xa0Ulmann, Analog and Hybrid Computer Programming.\\xa0\\xa0\\xa0De Gruyter, 2020.\\n\\n\\n\\n\\n[48]\\n\\nJ.\\xa0Hasler, “Large-scale field-programmable analog arrays,” Proceedings of the IEEE, vol. 108, no.\\xa08, pp. 1283–1302, 2020.\\n\\n\\n\\n\\n[49]\\n\\nE.\\xa0Donati, M.\\xa0Payvand, N.\\xa0Risi, R.\\xa0Krause, K.\\xa0Burelo, G.\\xa0Indiveri, T.\\xa0Dalgaty, and E.\\xa0Vianello, “Processing EMG signals using reservoir computing on an event-based neuromorphic system,” in 2018 IEEE Biomedical Circuits and Systems Conference (BioCAS).\\xa0\\xa0\\xa0IEEE, 2018.\\n\\n\\n\\n\\n[50]\\n\\nD.\\xa0Kirk, “Optimal control theory: an introduction,” 1970.\\n\\n\\n\\n\\n[51]\\n\\nM.\\xa0Parashar and S.\\xa0Hariri, “Autonomic computing: An overview,” in Lecture Notes in Computer Science.\\xa0\\xa0\\xa0Springer Berlin Heidelberg, 2005, pp. 257–269.\\n\\n\\n\\n\\n[52]\\n\\nC.\\xa0Mead, “Neuromorphic electronic systems,” Proceedings of the IEEE, vol.\\xa078, no.\\xa010, pp. 1629–1636, 1990.\\n\\n\\n\\n\\n[53]\\n\\nM.\\xa0Davies, A.\\xa0Wild, G.\\xa0Orchard, Y.\\xa0Sandamirskaya, G.\\xa0A.\\xa0F. Guerra, P.\\xa0Joshi, P.\\xa0Plank, and S.\\xa0R. Risbud, “Advancing neuromorphic computing with Loihi: A survey of results and outlook,” Proceedings of the IEEE, vol. 109, no.\\xa05, pp. 911–934, 2021.\\n\\n\\n\\n\\n[54]\\n\\nY.\\xa0Zhang, P.\\xa0Qu, Y.\\xa0Ji, W.\\xa0Zhang, G.\\xa0Gao, G.\\xa0Wang, S.\\xa0Song, G.\\xa0Li, W.\\xa0Chen, W.\\xa0Zheng, F.\\xa0Chen, J.\\xa0Pei, R.\\xa0Zhao, M.\\xa0Zhao, and L.\\xa0Shi, “A system hierarchy for brain-inspired computing,” Nature, vol. 586, no. 7829, pp. 378–384, 2020.\\n\\n\\n\\n\\n[55]\\n\\nA.\\xa0Basu, J.\\xa0Acharya, T.\\xa0Karnik, H.\\xa0Liu, H.\\xa0Li, J.-S. Seo, and C.\\xa0Song, “Low-power, adaptive neuromorphic systems: Recent progress and future directions,” IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol.\\xa08, no.\\xa01, pp. 6–27, 2018.\\n\\n\\n\\n\\n[56]\\n\\nJ.\\xa0Jordan, M.\\xa0Schmidt, W.\\xa0Senn, and M.\\xa0A. Petrovici, “Evolving interpretable plasticity for spiking networks,” eLife, vol.\\xa010, p. e66273, oct 2021.\\n\\n\\n\\n\\n[57]\\n\\nB.\\xa0Confavreux, E.\\xa0J. Agnes, F.\\xa0Zenke, T.\\xa0Lillicrap, and T.\\xa0P. Vogels, “A meta-learning approach to (re)discover plasticity rules that carve a desired function into a neural network,” 34th Conference on Neural Information Processing Systems (NeurIPS 2020), 2020.\\n\\n\\n\\n\\n[58]\\n\\nS.\\xa0J. Verzi, F.\\xa0Rothganger, O.\\xa0D. Parekh, T.-T. Quach, N.\\xa0E. Miner, C.\\xa0M. Vineyard, C.\\xa0D. James, and J.\\xa0B. Aimone, “Computing with spikes: The advantage of fine-grained timing,” Neural Computation, vol.\\xa030, no.\\xa010, pp. 2660–2690, 2018.\\n\\n\\n\\n\\n[59]\\n\\nK.\\xa0E. Hamilton, T.\\xa0M. Mintz, and C.\\xa0D. Schuman, “Spike-based primitives for graph algorithms,” arXiv preprint, 2019.\\n\\n\\n\\n\\n[60]\\n\\nJ.\\xa0D. Smith, W.\\xa0Severa, A.\\xa0J. Hill, L.\\xa0Reeder, B.\\xa0Franke, R.\\xa0B. Lehoucq, O.\\xa0D. Parekh, and J.\\xa0B. Aimone, “Solving a steady-state PDE using spiking networks and neuromorphic hardware,” in International Conference on Neuromorphic Systems 2020.\\xa0\\xa0\\xa0ACM, 2020.\\n\\n\\n\\n\\n[61]\\n\\nC.\\xa0Bartolozzi, G.\\xa0Indiveri, and E.\\xa0Donati, “Embodied neuromorphic intelligence,” Nature Communications, vol.\\xa013, no.\\xa01, 2022.\\n\\n\\n\\n\\n[62]\\n\\nR.\\xa0J. Douglas and K.\\xa0A. Martin, “Recurrent neuronal circuits in the neocortex,” Current Biology, vol.\\xa017, no.\\xa013, pp. R496–R500, 2007.\\n\\n\\n\\n\\n[63]\\n\\nE.\\xa0Neftci, J.\\xa0Binas, U.\\xa0Rutishauser, E.\\xa0Chicca, G.\\xa0Indiveri, and R.\\xa0J. Douglas, “Synthesizing cognition in neuromorphic electronic systems,” Proceedings of the National Academy of Sciences, vol. 110, no.\\xa037, pp. E3468–E3476, 2013.\\n\\n\\n\\n\\n[64]\\n\\nD.\\xa0Gutierrez-Galan, T.\\xa0Schoepe, J.\\xa0P. Dominguez-Morales, A.\\xa0Jimenez-Fernandez, E.\\xa0Chicca, and A.\\xa0Linares-Barranco, “An event-based digital time difference encoder model implementation for neuromorphic systems,” IEEE Transactions on Neural Networks and Learning Systems, pp. 1–15, 2021.\\n\\n\\n\\n\\n[65]\\n\\nR.\\xa0Krause, J.\\xa0J.\\xa0A. van Bavel, C.\\xa0Wu, M.\\xa0A. Vos, A.\\xa0Nogaret, and G.\\xa0Indiveri, “Robust neuromorphic coupled oscillators for adaptive pacemakers,” Scientific Reports, vol.\\xa011, no.\\xa01, 2021.\\n\\n\\n\\n\\n[66]\\n\\nC.\\xa0Eliasmith and C.\\xa0H. Anderson, Neural Engineering (Computational Neuroscience Series): Computational, Representation, and Dynamics in Neurobiological Systems.\\xa0\\xa0\\xa0Cambridge, MA, USA: MIT Press, 2002.\\n\\n\\n\\n\\n[67]\\n\\nT.\\xa0Bekolay, J.\\xa0Bergstra, E.\\xa0Hunsberger, T.\\xa0DeWolf, T.\\xa0C. Stewart, D.\\xa0Rasmussen, X.\\xa0Choo, A.\\xa0R. Voelker, and C.\\xa0Eliasmith, “Nengo: a python tool for building large-scale functional brain models,” Frontiers in Neuroinformatics, vol.\\xa07, 2014.\\n\\n\\n\\n\\n[68]\\n\\nJ.\\xa0B. Aimone, W.\\xa0Severa, and C.\\xa0M. Vineyard, “Composing neural algorithms with fugu,” in Proceedings of the International Conference on Neuromorphic Systems.\\xa0\\xa0\\xa0ACM, 2019.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGenerated  on Tue Oct 15 10:05:53 2024 by LaTeXML\\n\\n\\n\\n\\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.web import BeautifulSoupWebReader\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "reader = BeautifulSoupWebReader()\n",
    "\n",
    "#load_data ist eine Instanzmethode und kann nicht direkt von der Klasse (BeautifulSoupWebReader) aufgerufen werden\n",
    "documents = reader.load_data([\"https://arxiv.org/html/2410.22352v1\"])\n",
    "\n",
    "print(documents)\n",
    "#display(Markdown(f\"\\n{documents[0].text[:1000]}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d764c",
   "metadata": {},
   "source": [
    "Als Ergebnis bekommen wir bei Standardkonfiguration eine Liste mit einem großen Dokument, das die gesamte Webseite enthält. Damit das Sprachmodell jedoch richtig damit arbeiten kann, müssen wir das Dokument in kleinere Chunks zerlegen.  \n",
    "Dafür gibt es einen sogenannten `SentenceSplitter`, der diese Aufgabe erfüllen kann.  \n",
    "Wende ihn auf unser Dokument an.\n",
    "\n",
    "Dokumentation: [LlamaIndex Node Parser](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5073f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle ein SentenceSplitter Objekt und +bergebe dann die folgenden Parameter\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "chunk_size=256\n",
    "chunk_overlap=20\n",
    "node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# erstelle nun die nodes\n",
    "nodes = node_parser.get_nodes_from_documents(documents, show_progress=False)\n",
    "# Woran es gehakt hat: [Document(text=\"long text\")] nicht bewusst was dieser Übergabeparameter war und dass dieser mit\n",
    "# documents hätte ersetzt werden sollen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c5953",
   "metadata": {},
   "source": [
    "Jetzt kannst du wie in der Aufgabe mit der PDF die Nodes einem VectorStoreIndex erstellen und die Nodes übergeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c03f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 **Erste 3 Chunks aus dem Dokument:**\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 1:**\n",
       "\n",
       "Neuromorphic Programming: Emerging Directions for Brain-Inspired Hardware\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "I Introduction\n",
       "\n",
       "II Motivation\n",
       "\n",
       "Domain\n",
       "Plasticity\n",
       "Stochasticity\n",
       "Decentralization\n",
       "Unobservability\n",
       "\n",
       "\n",
       "\n",
       "III Theoretical framework\n",
       "\n",
       "III-A Computing with physical systems\n",
       "III-B Computer programs\n",
       "III-C Programming process\n",
       "III-D Languages and Paradigms\n",
       "\n",
       "\n",
       "\n",
       "IV Programming Paradigms\n",
       "\n",
       "IV-A Imperative programming\n",
       "IV-B Declarative\n",
       "IV-C Decentralized programming\n",
       "IV-D Automated programming\n",
       "IV-E Non-digital programming\n",
       "\n",
       "\n",
       "\n",
       "V Neuromorphic Programming\n",
       "\n",
       "Neuromorphic co-design\n",
       "Machine learning methods\n",
       "Online learning\n",
       "Evolutionary methods\n",
       "Spiking neuromorphic algorithms\n",
       "Neurocomputational primitives\n",
       "Higher abstractions\n",
       "\n",
       "\n",
       "VI Future approaches to programming brain-inspired hardware\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Neuromorphic Programming: Emerging Directions for Brain-Inspired Hardware"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 2:**\n",
       "\n",
       "Neuromorphic Programming: Emerging Directions for Brain-Inspired Hardware\n",
       "\n",
       "\n",
       "Steven Abreu\n",
       "\n",
       "CogniGron Center & Bernoulli Institute\n",
       "University of Groningen\n",
       "Groningen, Netherlands \n",
       "s.abreu@rug.nl\n",
       "\n",
       "  \n",
       "Jens E. Pedersen\n",
       "\n",
       "Computational Science and Technology\n",
       "KTH Royal Institute of Technology\n",
       "Stockholm, Sweden \n",
       "jeped@kth.se"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Chunk 3:**\n",
       "\n",
       "Abstract\n",
       "The value of brain-inspired neuromorphic computers critically depends on our ability to program them for relevant tasks. Currently, neuromorphic hardware often relies on machine learning methods adapted from deep learning.\n",
       "However, neuromorphic computers have potential far beyond deep learning if we can only harness their energy efficiency and full computational power.\n",
       "Neuromorphic programming will necessarily be different from conventional programming, requiring a paradigm shift in how we think about programming.\n",
       "This paper presents a conceptual analysis of programming within the context of neuromorphic computing, challenging conventional paradigms and proposing a framework that aligns more closely with the physical intricacies of these systems.\n",
       "Our analysis revolves around five characteristics that are fundamental to neuromorphic programming and provides a basis for comparison to contemporary programming methods and languages.\n",
       "By studying past approaches, we contribute a framework that advocates for underutilized techniques and calls for richer abstractions to effectively instrument the new hardware class.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "Index Terms: \n",
       "neuromorphic computing, brain-inspired computing, hardware-software co-design, programming techniques\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "I Introduction"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VectorStoreIndex Objekt erstellen und die nodes übergeben\n",
    "\n",
    "index = VectorStoreIndex(nodes)\n",
    "#oder index = VectorStoreIndex.from_documents(documents, transformations=[node_parser],)?\n",
    "\n",
    "\n",
    "print(\"\\n🔍 **Erste 3 Chunks aus dem Dokument:**\\n\")\n",
    "for i, node in enumerate(nodes[:3], 1):\n",
    "    display(Markdown(f\"**Chunk {i}:**\\n\\n{node.text}\"))\n",
    "\n",
    "    \n",
    "#print(index)\n",
    "#print(nodes)\n",
    "#print(\"fertig\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90d0a7",
   "metadata": {},
   "source": [
    "Abschließend können wir eine Query-Engine erstellen, die die folgenden Schritte ausführt:\n",
    "\n",
    "1. Die passenden Nodes basierend auf der Anfrage suchen.\n",
    "2. Diese Nodes zusammen mit der Anfrage an das Sprachmodell übergeben.\n",
    "3. Das Sprachmodell liefert eine Antwort, die die Anfrage und die zusätzlichen Informationen berücksichtigt.\n",
    "\n",
    "Hierfür kann die Funktion `.as_query_engine` verwendet werden.\n",
    "\n",
    "Dokumentation: [Query Engine Dokumentation](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d1453668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initalisiere die Query Engine\n",
    "query_engine = index.as_query_engine(streaming=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227522b5",
   "metadata": {},
   "source": [
    "Nun frage die Query Engine die Folgenden Fragen und lasse sowohl die Antwort, als auch die beigefügten Nodes vinnvoll ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21ad4b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-digital programming bezieht sich auf die Erstellung von Computenprogrammen ohne direkte Verwendung eines digitalen Rechensystems. Es umfasst das Erstellen von Programmcode für physische Systeme, die über traditionelle digitale Computer hinausgehen, wie z.B. analogen Schaltkreise oder physikalische Substrate."
     ]
    }
   ],
   "source": [
    "#Frage: Was bedeutet Non-digital programming?\n",
    "streaming_response = query_engine.query(\"Was bedeutet Non-digital programming\")\n",
    "\n",
    "#Output\n",
    "streaming_response.print_response_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1cbccb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decentralized programming ermöglicht die Ausführung von mehreren Programmprozessen gleichzeitig und gleichzeitig. Die Lebensdauer verschiedener Rechenprozesse überschneidet sich und kann auch miteinander interagieren. Diese Art der Programmierung führt zu Problemen mit Synchronisation, wie z.B. Abhängen und Rennen."
     ]
    }
   ],
   "source": [
    "#Frage: Was bedeutet Decentralized programming?\n",
    "streaming_response = query_engine.query(\"Was bedeutet Decentralized programming\")\n",
    "\n",
    "#Output\n",
    "streaming_response.print_response_stream()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f19ea8-8e8b-48bc-b9aa-654e86cd2549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
